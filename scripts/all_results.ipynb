{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row buffer misses per kilo instruction (RBMPKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ RESULTS INTO PANDAS DATAFRAME\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsdir = \"../results\"\n",
    "# list all directories in resultsdir\n",
    "configs = [d for d in os.listdir(resultsdir) if os.path.isdir(os.path.join(resultsdir, d))]\n",
    "configs = ['Baseline.yaml']\n",
    "# print found configs\n",
    "print('Found configs: {}'.format(configs))\n",
    "# list all directories under all configs\n",
    "workloads = []\n",
    "for c in configs:\n",
    "    workloads.append([d for d in os.listdir(os.path.join(resultsdir, c)) if os.path.isdir(os.path.join(resultsdir, c, d))])\n",
    "# find only the intersection of all workloads\n",
    "workloads = list(set.intersection(*map(set, workloads)))\n",
    "# print found workloads\n",
    "print('Found workloads: {}'.format(workloads))\n",
    "\n",
    "stats_per_config_workload = []\n",
    "\n",
    "\n",
    "# for every config + workload directory\n",
    "for c in configs:\n",
    "    for w in workloads:\n",
    "        # find all files in the directory\n",
    "        files = [f for f in os.listdir(os.path.join(resultsdir, c, w)) if os.path.isfile(os.path.join(resultsdir, c, w, f))]\n",
    "        # find the stats file\n",
    "        stat_files = [f for f in files if f.endswith('.stats')]\n",
    "        # if there is a stats file\n",
    "        if stat_files:\n",
    "            for stat_file in stat_files:\n",
    "                # if the stats_file has less than three lines skip it\n",
    "                if len(open(os.path.join(resultsdir, c, w, stat_file)).readlines()) < 3:\n",
    "                    continue\n",
    "\n",
    "                extension = ''\n",
    "                # if stats_file file name itself does not start with DDR4, parse it a bit\n",
    "                if not stat_file.startswith('DDR4'):\n",
    "                    # get the config name from the stats_file name\n",
    "                    extension = '_'.join(stat_file.split('_')[:-1])\n",
    "                    # prepend underscore to extension\n",
    "                    extension = '_' + extension\n",
    "\n",
    "                # read the stats file, name columns: 'name', 'value', 'description'\n",
    "                df = pd.read_csv(os.path.join(resultsdir, c, w, stat_file), header=None).T\n",
    "                df.columns = df.iloc[0]\n",
    "                df.drop(0,inplace=True)\n",
    "                # add a new column called 'config' with the config name\n",
    "                df['config'] = c + extension\n",
    "                # add a new column called 'workload' with the workload name\n",
    "                df['workload'] = w\n",
    "                # print the stats file\n",
    "                # print('Config: {}, Workload: {}, Stats: {}'.format(c, w, df))\n",
    "                # append the stats to the list\n",
    "                df.reset_index(inplace=True, drop=True)\n",
    "                stats_per_config_workload.append(df)\n",
    "        else:\n",
    "            print('Config: {}, Workload: {}, Stats: No stats file found'.format(c, w))\n",
    "\n",
    "# concatenate all stats into one dataframe\n",
    "stats = pd.concat(stats_per_config_workload)\n",
    "\n",
    "# find elements where workload does not contain '-'\n",
    "# these are single core workloads\n",
    "stats = stats[~stats['workload'].str.contains('-')]\n",
    "\n",
    "# calculate row buffer misses per kilo instruction (conflicts + misses)\n",
    "\n",
    "#record_read_conflictstotal record_read_missestotal\n",
    "#record_write_conflictstotal record_write_missestotal\n",
    "\n",
    "stats['ramulator.rbmpki'] = (stats['ramulator.row_conflicts_channel_0_core'] + stats['ramulator.row_misses_channel_0_core']) /\\\n",
    "                            stats['ramulator.record_insts_core_0'] * 1000\n",
    "                            \n",
    "stats['ramulator.mpki'] = (stats['ramulator.L3_cache_total_miss'] - stats['ramulator.L3_cache_mshr_unavailable']) /\\\n",
    "                            stats['ramulator.record_insts_core_0'] * 1000\n",
    "\n",
    "# group workloads according to their rbmpki and show their names. Use three bins [0,2] [2,10] [10+]\n",
    "# do not add to the dataframe, just print the results\n",
    "print('MPKI per workload')\n",
    "stats.sort_values(by=['ramulator.mpki'], inplace=True)\n",
    "\n",
    "# print workload and mpki one by one\n",
    "\n",
    "for index, row in stats.iterrows():\n",
    "    print('{} = {}'.format(row['workload'], row['ramulator.mpki']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation - Figures 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL Pre-processing\n",
    "\n",
    "Need to do once if you want to get per-workload sibling row activation distribution from raw data. This will take a lot of time and use a lot of memory.\n",
    "\n",
    "We generate the raw data for this analysis using the \"ACT-period-256ms.yaml\" configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative_bank_usage_stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# one counter per DRAM row, in each counter, there are as many entries as there are ranks + banks, which is 32\n",
    "def find_distribution_bank_usage_stat(filepath, analysis_threshold):\n",
    "\n",
    "  counters = {}\n",
    "\n",
    "  bank_usage_stats = []\n",
    "\n",
    "  actual_lines = []\n",
    "  with open(filepath, \"r\") as f:\n",
    "      for line in f:\n",
    "          actual_lines = line.split(' ')\n",
    "          break\n",
    "        \n",
    "  for i in range (len(actual_lines)):\n",
    "    # if line is empty, skip\n",
    "    if actual_lines[i] == \"\":\n",
    "      continue\n",
    "    \n",
    "    line = actual_lines[i]\n",
    "\n",
    "    tokens = line.split(\":\")\n",
    "    ra = int(tokens[1])\n",
    "    bg = int(tokens[2])\n",
    "    ba = int(tokens[3])\n",
    "    row = int(tokens[4])\n",
    "    \n",
    "    secondary_index = ra << 4 | bg << 2 | ba\n",
    "    \n",
    "    if row not in counters:\n",
    "      init_array = []\n",
    "      for i in range(32):\n",
    "        init_array.append(0)\n",
    "      counters[row] = init_array\n",
    "      counters[row][secondary_index] += 1\n",
    "    else:\n",
    "      counters[row][secondary_index] += 1\n",
    "      if counters[row][secondary_index] == analysis_threshold:\n",
    "        # append all counter values except secondary_index to bank_usage_stats\n",
    "        for j in range(len(counters[row])):\n",
    "          if j != secondary_index:\n",
    "            bank_usage_stats.append(counters[row][j])\n",
    "               \n",
    "        # remove this row from the counters\n",
    "        del counters[row]\n",
    "        \n",
    "    # print the progress of lines so far as fraction of total lines\n",
    "    if i % 1000000 == 0:\n",
    "      print(\"Progress:\", str(i / len(actual_lines)))\n",
    "  \n",
    "  return bank_usage_stats\n",
    "\n",
    "# all workloads are in activate-periods directory\n",
    "# under each workload directory, there is an activate_periods.txt file\n",
    "\n",
    "MED_RBMPKI = ['510.parest', '462.libquantum', 'tpch2', 'wc_8443', 'ycsb_aserver', '473.astar', 'stream_10.trace', 'jp2_decode', '436.cactusADM', '557.xz', 'ycsb_cserver', 'ycsb_eserver', '471.omnetpp', '483.xalancbmk', '505.mcf', 'wc_map0', 'jp2_encode', 'tpch17', 'ycsb_bserver', 'tpcc64', '482.sphinx3']\n",
    "HIGH_RBMPKI = ['519.lbm', '459.GemsFDTD', '450.soplex', 'h264_decode', '520.omnetpp', '433.milc', '434.zeusmp', 'random_10.trace', 'bfs_dblp', '429.mcf', '549.fotonik3d', '470.lbm', 'bfs_ny', 'bfs_cm2003', '437.leslie3d']\n",
    "MEDHIGH_RBMPKI = MED_RBMPKI + HIGH_RBMPKI\n",
    "\n",
    "# iterate over all workloads\n",
    "for workload in MEDHIGH_RBMPKI:\n",
    "  df = pd.DataFrame(columns=['workload', 'cumulative_bank_usage_stat', 'analysis_threshold'])\n",
    "  # iterate over all analysis_thresholds\n",
    "  for analysis_threshold in [2, 125, 250, 500]:\n",
    "    all_bank_usage_stats = find_distribution_bank_usage_stat(\"activate_periods/\" + workload + \"/activate_periods.txt\", analysis_threshold)\n",
    "    workload_array = [workload for i in range(len(all_bank_usage_stats))]\n",
    "    analysis_threshold_array = [analysis_threshold for i in range(len(all_bank_usage_stats))]\n",
    "    # add these to df fast\n",
    "    df = pd.concat([df, pd.DataFrame({'workload': workload_array, 'analysis_threshold': analysis_threshold_array, 'bank_usage_stat': all_bank_usage_stats})])    \n",
    "    print(\"workload:\", workload, \"analysis_threshold:\", analysis_threshold)  \n",
    "  # save df as csv\n",
    "  df.to_csv(\"distr_bank_usage_stat_\" + workload + \".csv\")\n",
    "  \n",
    "\n",
    "# empty dataframe with the same columns\n",
    "dfn = pd.DataFrame(columns=['workload', 'cumulative_bank_usage_stat', 'analysis_threshold', 'bank_usage_stat'])\n",
    "\n",
    "# Add RH_ESTIMATE workloads to the dataframe\n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds', 0, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfn = pd.concat([dfn, pd.DataFrame([['ds-p1', 124, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 31):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p1', 0, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 8):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p8', 124, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 24):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p8', 0, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])  \n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p32', 124, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "\n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms', 0, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfn = pd.concat([dfn, pd.DataFrame([['ms-p1', 124, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 31):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p1', 0, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 8):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p8', 124, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 24):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p8', 0, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])  \n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p32', 124, 125]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "\n",
    "# Add RH_ESTIMATE workloads to the dataframe\n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds', 0, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfn = pd.concat([dfn, pd.DataFrame([['ds-p1', 249, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 31):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p1', 0, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 8):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p8', 249, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 24):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p8', 0, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])  \n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p32', 249, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "\n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms', 0, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfn = pd.concat([dfn, pd.DataFrame([['ms-p1', 249, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 31):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p1', 0, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 8):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p8', 249, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 24):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p8', 0, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])  \n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p32', 249, 250]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "\n",
    "# Add RH_ESTIMATE workloads to the dataframe\n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds', 0, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfn = pd.concat([dfn, pd.DataFrame([['ds-p1', 499, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 31):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p1', 0, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 8):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p8', 499, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 24):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p8', 0, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])  \n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ds-p32', 499, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "\n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms', 0, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfn = pd.concat([dfn, pd.DataFrame([['ms-p1', 499, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 31):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p1', 0, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 8):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p8', 499, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "for i in range (0, 24):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p8', 0, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])  \n",
    "for i in range (0, 32):\n",
    "  dfn = pd.concat([dfn, pd.DataFrame([['ms-p32', 499, 500]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])  \n",
    "  \n",
    "# read all csvs into one dataframe\n",
    "# read one file at a time, store it in temporary dataframe\n",
    "# sample 1/10 of the data randomly to put in the final datafrmae\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# read all csvs into one dataframe\n",
    "# read one file at a time, store it in temporary dataframe\n",
    "# sample 1/10 of the data randomly to put in the final datafrmae\n",
    "df = pd.DataFrame()\n",
    "for filename in glob.glob(\"distr_bank_usage_stat_*.csv\"):\n",
    "    df_tmp = pd.read_csv(filename, index_col=None, header=0)\n",
    "    # sample 1/10 of the data\n",
    "    df_tmp = df_tmp.sample(frac=0.1, random_state=1)\n",
    "    # add to final dataframe using concat\n",
    "    df = pd.concat([df, df_tmp])    \n",
    "    print(\"Done reading \" + filename)\n",
    "\n",
    "# merge dfn to df\n",
    "df = pd.concat([df, dfn])\n",
    "\n",
    "df.to_csv(\"distr_bank_usage.csv\", index=False)\n",
    "\n",
    "\n",
    "# read all csvs into one dataframe\n",
    "# read one file at a time, store it in temporary dataframe\n",
    "# sample 1/10 of the data randomly to put in the final datafrmae\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "# read all csvs into one dataframe\n",
    "# read one file at a time, store it in temporary dataframe\n",
    "# sample 1/10 of the data randomly to put in the final datafrmae\n",
    "df = pd.DataFrame()\n",
    "new_df_arr = []\n",
    "for filename in glob.glob(\"distr_bank_usage_stat_*.csv\"):\n",
    "    print(filename)\n",
    "    df_tmp = pd.read_csv(filename, index_col=0, header=0)\n",
    "    df_tmp = df_tmp[df_tmp['analysis_threshold'] == 2]\n",
    "    # select bank_usage_stat as series\n",
    "    sum_series = df_tmp['bank_usage_stat']\n",
    "    # sum of every 31 elements\n",
    "    sum_series = sum_series.groupby(np.arange(len(sum_series))//31).sum()\n",
    "    # reconstruct a df from the series\n",
    "    df_new = pd.DataFrame({'bank_usage_stat': sum_series})\n",
    "    df_new['workload'] = df_tmp['workload'][0]\n",
    "    df_new['analysis_threshold'] = df_tmp['analysis_threshold'][0]\n",
    "    new_df_arr.append(df_new)\n",
    "    # print size in MB of df_new\n",
    "    print(df_new.memory_usage().sum() / 1024**2)\n",
    "\n",
    "# concat new_df_arr\n",
    "df = pd.concat(new_df_arr)\n",
    "# merge dfn to df\n",
    "df = pd.concat([df, dfn])\n",
    "\n",
    "df.to_csv(\"distr_bank_usage_first_plot.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative_bank_usage_stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../results/distr_bank_usage_first_plot.csv\")\n",
    "\n",
    "MED_RBMPKI = ['510.parest', '462.libquantum', 'tpch2', 'wc_8443', 'ycsb_aserver', '473.astar', 'stream_10.trace', 'jp2_decode', '436.cactusADM', '557.xz', 'ycsb_cserver', 'ycsb_eserver', '471.omnetpp', '483.xalancbmk', '505.mcf', 'wc_map0', 'jp2_encode', 'tpch17', 'ycsb_bserver', 'tpcc64', '482.sphinx3']\n",
    "HIGH_RBMPKI = ['519.lbm', '459.GemsFDTD', '450.soplex', 'h264_decode', '520.omnetpp', '433.milc', '434.zeusmp', 'bfs_dblp', '429.mcf', '549.fotonik3d', 'random_10.trace', 'gups', '470.lbm', 'bfs_ny', 'bfs_cm2003', '437.leslie3d']\n",
    "RH_ESTIMATE = ['ds', 'ds-p1' , 'ds-p8', 'ds-p32', 'ms' , 'ms-p1', 'ms-p8', 'ms-p32']\n",
    "# dfx = df.copy()\n",
    "dfx = df\n",
    "\n",
    "\n",
    "\n",
    "# Add RH_ESTIMATE workloads to the dataframe\n",
    "dfx = pd.concat([dfx, pd.DataFrame([['ds', 0, 2]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfx = pd.concat([dfx, pd.DataFrame([['ds-p1', 1, 2]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfx = pd.concat([dfx, pd.DataFrame([['ds-p8', 8, 2]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfx = pd.concat([dfx, pd.DataFrame([['ds-p32', 31, 2]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfx = pd.concat([dfx, pd.DataFrame([['ms', 0, 2]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfx = pd.concat([dfx, pd.DataFrame([['ms-p1', 1, 2]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfx = pd.concat([dfx, pd.DataFrame([['ms-p8', 8, 2]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "dfx = pd.concat([dfx, pd.DataFrame([['ms-p32', 31, 2]], columns=['workload', 'bank_usage_stat', 'analysis_threshold'])])\n",
    "\n",
    "MEDHIGH_RBMPKI = MED_RBMPKI + HIGH_RBMPKI + RH_ESTIMATE\n",
    "# remove where workload is stream_10.trace and random_10.trace from MEDHIGH_RBMPKI\n",
    "MEDHIGH_RBMPKI.remove('stream_10.trace')\n",
    "MEDHIGH_RBMPKI.remove('gups')\n",
    "# MEDHIGH_RBMPKI.remove('random_10.trace')\n",
    "# get rid of workloads not in MEDHIGH_RBMPKI\n",
    "dfn = dfx[dfx['workload'].isin(MEDHIGH_RBMPKI)]\n",
    "# dfn['use_stat'] = dfn['cumulative_bank_usage_stat'] * 32\n",
    "# remove where workload is stream_10.trace and random_10.trace\n",
    "dfn = dfn[dfn['workload'] != 'stream_10.trace']\n",
    "# dfn = dfn[dfn['workload'] != 'random_10.trace']\n",
    "dfn = dfn[dfn['workload'] != 'gups']\n",
    "\n",
    "dfn['workload'] = dfn['workload'].replace('random_10.trace', 'gups')\n",
    "\n",
    "# #rename random_10.trace to gups in MEDHIGH_RBMPKI\n",
    "MEDHIGH_RBMPKI = [x.replace('random_10.trace', 'gups') for x in MEDHIGH_RBMPKI]\n",
    "order = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp', 'ds', 'ds-p1' , 'ds-p8', 'ds-p32', 'ms' , 'ms-p1', 'ms-p8', 'ms-p32']\n",
    "# remove from order, workloads not in MEDHIGH_RBMPKI\n",
    "order = [x for x in order if x in MEDHIGH_RBMPKI]\n",
    "# sort by workload according to order\n",
    "dfn['workload'] = pd.Categorical(dfn['workload'], order)\n",
    "dfn = dfn.sort_values('workload')\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "# create side-by-side two subplots\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(7, 1.5))\n",
    "\n",
    "PROPS = {\n",
    "    'boxprops':{'edgecolor':'black'},\n",
    "    'medianprops':{'color':'black'},\n",
    "    'whiskerprops':{'color':'black'},\n",
    "    'capprops':{'color':'black'}\n",
    "}\n",
    "\n",
    "\n",
    "flierprops = dict(markerfacecolor='black', markeredgecolor='black', markersize=2,\n",
    "              linestyle='none')\n",
    "\n",
    "# barplot workload on x axis, cumulative_bank_usage_stat on y axis, only for analysis_threshold = 4, only for workloads with high RBMPKI\n",
    "sns.boxplot(ax=ax1, x=\"workload\", y=\"bank_usage_stat\", data=dfn[(dfn['analysis_threshold'] == 2) & (dfn['workload'].isin(MEDHIGH_RBMPKI))], palette=\"pastel\", showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\", \"markersize\":\"3\"}, flierprops=flierprops,**PROPS)\n",
    "# rotate x axis labels\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\n",
    "# rename x axis\n",
    "ax1.set_xlabel(\"Workload\")\n",
    "# rename y axis\n",
    "ax1.set_ylabel(\"Number of\\nsibling row activations\")\n",
    "\n",
    "\n",
    "# show y until 4 for ax1\n",
    "ax1.set_ylim([0, 35.84])\n",
    "# y ticks in increments of 1 for ax1\n",
    "ax1.set_yticks(np.arange(0, 32, 8).tolist() + [31]) \n",
    "# draw a red line at 4\n",
    "ax1.axhline(y=31, color='r', linestyle='-')\n",
    "# move y axis labels down\n",
    "ax1.yaxis.set_label_coords(-0.08,0.1)\n",
    "\n",
    "# make x axis tick labels smaller\n",
    "ax1.tick_params(axis='x', labelsize=10)\n",
    "\n",
    "# draw verticel line between leslie3d and ds\n",
    "ax1.axvline(x=34.5, color='black', linestyle='--')\n",
    "# continue the line below drawing area\n",
    "\n",
    "# add text to the right of the vertical line\n",
    "ax1.text(38.7, -30, 'RowHammer\\nAttacks', fontsize=9, va='center', ha='center', color='brown')\n",
    "\n",
    "# color the last 8 x axis tick labels blue\n",
    "for tick in ax1.get_xticklabels()[-8:]:\n",
    "  tick.set_color('brown')\n",
    "\n",
    "\n",
    "# save as pdf tight layout\n",
    "plt.savefig(\"sibling_row_activations.pdf\", bbox_inches='tight')\n",
    "\n",
    "# print average use_stat across workloads\n",
    "# print(\"Average use_stat across workloads:\", dfn[(dfn['analysis_threshold'] == 2)]['use_stat'].mean())\n",
    "# # max and min\n",
    "# print(\"Max use_stat across workloads:\", dfn[(dfn['analysis_threshold'] == 2)]['use_stat'].max())\n",
    "# print(\"Min use_stat across workloads:\", dfn[(dfn['analysis_threshold'] == 2)]['use_stat'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative_bank_usage_stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../results/distr_bank_usage.csv\")\n",
    "\n",
    "MED_RBMPKI = ['510.parest', '462.libquantum', 'tpch2', 'wc_8443', 'ycsb_aserver', '473.astar', 'stream_10.trace', 'jp2_decode', '436.cactusADM', '557.xz', 'ycsb_cserver', 'ycsb_eserver', '471.omnetpp', '483.xalancbmk', '505.mcf', 'wc_map0', 'jp2_encode', 'tpch17', 'ycsb_bserver', 'tpcc64', '482.sphinx3']\n",
    "HIGH_RBMPKI = ['519.lbm', '459.GemsFDTD', '450.soplex', 'h264_decode', '520.omnetpp', '433.milc', '434.zeusmp', 'bfs_dblp', '429.mcf', '549.fotonik3d', 'random_10.trace', '470.lbm', 'bfs_ny', 'bfs_cm2003', '437.leslie3d']\n",
    "MEDHIGH_RBMPKI = MED_RBMPKI + HIGH_RBMPKI\n",
    "RH_ESTIMATE = ['ds', 'ds-p1' , 'ds-p8', 'ds-p32', 'ms' , 'ms-p1', 'ms-p8', 'ms-p32']\n",
    "\n",
    "MEDHIGH_RBMPKI = MED_RBMPKI + HIGH_RBMPKI + RH_ESTIMATE\n",
    "\n",
    "# remove where workload is stream_10.trace and random_10.trace from MEDHIGH_RBMPKI\n",
    "MEDHIGH_RBMPKI.remove('stream_10.trace')\n",
    "# MEDHIGH_RBMPKI.remove('random_10.trace')\n",
    "# MEDHIGH_RBMPKI.remove('gups')\n",
    "# get rid of workloads not in MEDHIGH_RBMPKI\n",
    "dfn = df[df['workload'].isin(MEDHIGH_RBMPKI)].copy()\n",
    "# remove where workload is stream_10.trace and random_10.trace\n",
    "dfn = dfn[dfn['workload'] != 'stream_10.trace']\n",
    "# dfn = dfn[dfn['workload'] != 'random_10.trace']\n",
    "dfn = dfn[dfn['workload'] != 'gups']\n",
    "\n",
    "# rename random_10.trace to gups\n",
    "dfn['workload'] = dfn['workload'].replace('random_10.trace', 'gups')\n",
    "\n",
    "#rename random_10.trace to gups in MEDHIGH_RBMPKI\n",
    "MEDHIGH_RBMPKI = [x.replace('random_10.trace', 'gups') for x in MEDHIGH_RBMPKI]\n",
    "\n",
    "order = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp', 'ds', 'ds-p1' , 'ds-p8', 'ds-p32', 'ms' , 'ms-p1', 'ms-p8', 'ms-p32']\n",
    "# remove from order, workloads not in MEDHIGH_RBMPKI\n",
    "order = [x for x in order if x in MEDHIGH_RBMPKI]\n",
    "# sort by workload according to order\n",
    "dfn['workload'] = pd.Categorical(dfn['workload'], order)\n",
    "dfn = dfn.sort_values('workload')\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "# create side-by-side two subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, figsize=(7, 4.5), sharex=True)\n",
    "\n",
    "PROPS = {\n",
    "    'boxprops':{'edgecolor':'black'},\n",
    "    'medianprops':{'color':'black'},\n",
    "    'whiskerprops':{'color':'black'},\n",
    "    'capprops':{'color':'black'}\n",
    "}\n",
    "\n",
    "flierprops = dict(markerfacecolor='black', markeredgecolor='black', markersize=2,\n",
    "              linestyle='none')\n",
    "\n",
    "# barplot workload on x axis, cumulative_bank_usage_stat on y axis, only for analysis_threshold = 4, only for workloads with high RBMPKI\n",
    "sns.boxplot(ax=ax3, x=\"workload\", y=\"bank_usage_stat\", data=dfn[(dfn['analysis_threshold'] == 125) & (dfn['workload'].isin(MEDHIGH_RBMPKI))], palette=\"pastel\", showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\", \"markersize\":\"3\"}, flierprops=flierprops,**PROPS)\n",
    "# rotate x axis labels\n",
    "\n",
    "# rename x axis\n",
    "ax3.set_xlabel(\"Workload\")\n",
    "\n",
    "# rename y axis\n",
    "ax3.set_ylabel(\"Average Counter Value\")\n",
    "\n",
    "# plot the same but for analysis_threshold = 32\n",
    "sns.boxplot(ax=ax2, x=\"workload\", y=\"bank_usage_stat\", data=dfn[(dfn['analysis_threshold'] == 250) & (dfn['workload'].isin(MEDHIGH_RBMPKI))], palette=\"pastel\", showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\", \"markersize\":\"3\"},flierprops=flierprops, **PROPS)\n",
    "# rotate x axis labels\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=90)\n",
    "ax2.set_xlabel(\"\")\n",
    "ax2.set_ylabel(\"Average Counter Value\")\n",
    "\n",
    "\n",
    "# plot the same but for analysis_threshold = 32\n",
    "sns.boxplot(ax=ax1, x=\"workload\", y=\"bank_usage_stat\", data=dfn[(dfn['analysis_threshold'] == 500) & (dfn['workload'].isin(MEDHIGH_RBMPKI))], palette=\"pastel\", showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\", \"markersize\":\"3\"},flierprops=flierprops, **PROPS)\n",
    "# rotate x axis labels\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_ylabel(\"Sibling row activation count\")\n",
    "\n",
    "ax3.set_xlabel(\"Workload\")\n",
    "ax3.set_xticklabels(ax3.get_xticklabels(), rotation=90)\n",
    "\n",
    "# set x axis tick labels using workload names\n",
    "ax3.set_xticklabels(order)\n",
    "\n",
    "\n",
    "# show y until 4 for ax1\n",
    "ax3.set_ylim([0, 140])\n",
    "# y ticks in increments of 1 for ax1\n",
    "ax3.set_yticks(np.arange(0, 125, 32).tolist() + [125])\n",
    "# draw a red line at 4\n",
    "ax3.axhline(y=125, color='r', linestyle='-')\n",
    "ax2.set_ylim([0, 280])\n",
    "# y ticks in increments of 1 for ax1\n",
    "ax2.set_yticks(np.arange(0, 249, 64).tolist() + [250])\n",
    "# draw a red line at (analysis_threshold-1)\n",
    "ax2.axhline(y=250, color='r', linestyle='-')\n",
    "# show y until 4 for ax1\n",
    "ax1.set_ylim([0, 560])\n",
    "# y ticks in increments of 1 for ax1\n",
    "ax1.set_yticks(np.arange(0, 500, 128).tolist() + [500])\n",
    "# draw a red line at (analysis_threshold-1)\n",
    "ax1.axhline(y=500, color='r', linestyle='-')\n",
    "\n",
    "# show y until 4 for ax1\n",
    "\n",
    "# move y axis labels down\n",
    "ax1.yaxis.set_label_coords(-0.12,-0.8)\n",
    "ax2.yaxis.set_label_coords(-0.09,0.3)\n",
    "ax3.yaxis.set_label_coords(-0.09,0.3)\n",
    "\n",
    "# make x axis tick labels smaller\n",
    "ax1.tick_params(axis='x', labelsize=10)\n",
    "ax2.tick_params(axis='x', labelsize=10)\n",
    "ax3.tick_params(axis='x', labelsize=10)\n",
    "\n",
    "# remove ax2-3 y axis labels\n",
    "ax2.set_ylabel('')\n",
    "ax3.set_ylabel('')\n",
    "\n",
    "\n",
    "# title each ax\n",
    "ax3.set_title(\"RowHammer Threshold $(N_{RH})$ = 125\")\n",
    "ax2.set_title(\"RowHammer Threshold $(N_{RH})$ = 250\")\n",
    "ax1.set_title(\"RowHammer Threshold $(N_{RH})$ = 500\")\n",
    "\n",
    "# draw verticel line between leslie3d and ds\n",
    "ax1.axvline(x=34.5, color='black', linestyle='--')\n",
    "ax2.axvline(x=34.5, color='black', linestyle='--')\n",
    "ax3.axvline(x=34.5, color='black', linestyle='--')\n",
    "# continue the line below drawing area\n",
    "# add text to the right of the vertical line\n",
    "ax3.text(38.7, -140, 'RowHammer\\nAttacks', fontsize=9, va='center', ha='center', color='brown')\n",
    "\n",
    "# color the last 8 x axis tick labels blue\n",
    "for tick in ax1.get_xticklabels()[-8:]:\n",
    "  tick.set_color('brown')\n",
    "# color the last 8 x axis tick labels blue\n",
    "for tick in ax2.get_xticklabels()[-8:]:\n",
    "  tick.set_color('brown')\n",
    "# color the last 8 x axis tick labels blue\n",
    "for tick in ax3.get_xticklabels()[-8:]:\n",
    "  tick.set_color('brown')\n",
    "\n",
    "# reduce gap between subplots\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "# save as pdf tight layout\n",
    "plt.savefig(\"distr_counter_values.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance and Energy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 8 and 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MECHANISM_NAME = 'ABACuS'\n",
    "\n",
    "### READ RESULTS INTO PANDAS DATAFRAME\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsdir = \"../results\"\n",
    "# list all directories in resultsdir\n",
    "configs = [d for d in os.listdir(resultsdir) if os.path.isdir(os.path.join(resultsdir, d))]\n",
    "configs = ['Baseline.yaml', 'Hydra-Baseline.yaml', 'REGA125.yaml', 'REGA250.yaml', 'REGA500.yaml', 'REGA1000.yaml', 'Graphene125.yaml', 'Graphene250.yaml', 'Graphene500.yaml', 'Graphene1000.yaml', 'Hydra125.yaml', 'Hydra250.yaml', 'Hydra500.yaml', 'Hydra1000.yaml', 'PARA125.yaml', 'PARA250.yaml', 'PARA500.yaml', 'PARA1000.yaml', \n",
    "           'ABACUS125.yaml', 'ABACUS250.yaml', 'ABACUS500.yaml', 'ABACUS1000.yaml', 'ABACUS125-Big.yaml', 'ABACUS250-Big.yaml', 'ABACUS500-Big.yaml', 'ABACUS1000-Big.yaml']\n",
    "# print found configs\n",
    "print('Found configs: {}'.format(configs))\n",
    "# list all directories under all configs\n",
    "workloads = []\n",
    "for c in configs:\n",
    "    workloads.append([d for d in os.listdir(os.path.join(resultsdir, c)) if os.path.isdir(os.path.join(resultsdir, c, d))])\n",
    "# find only the intersection of all workloads\n",
    "workloads = list(set.intersection(*map(set, workloads)))\n",
    "# print found workloads\n",
    "print('Found workloads: {}'.format(workloads))\n",
    "\n",
    "stats_per_config_workload = []\n",
    "workloads = [w for w in workloads if not '-' in w]\n",
    "\n",
    "# for every config + workload directory\n",
    "for c in configs:\n",
    "    for w in workloads:\n",
    "        # find all files in the directory\n",
    "        files = [f for f in os.listdir(os.path.join(resultsdir, c, w)) if os.path.isfile(os.path.join(resultsdir, c, w, f))]\n",
    "        # find the stats file\n",
    "        stat_files = [f for f in files if f.endswith('.stats')]\n",
    "        # if there is a stats file\n",
    "        if stat_files:\n",
    "            for stat_file in stat_files:\n",
    "                # if the stats_file has less than three lines skip it\n",
    "                if len(open(os.path.join(resultsdir, c, w, stat_file)).readlines()) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # print the name of the stats_file\n",
    "                print('Found stats file: {}'.format(os.path.join(os.path.join(resultsdir, c, w, stat_file))))\n",
    "\n",
    "                extension = ''\n",
    "                # if stats_file file name itself does not start with DDR4, parse it a bit\n",
    "                if not stat_file.startswith('DDR4'):\n",
    "                    # get the config name from the stats_file name\n",
    "                    extension = '_'.join(stat_file.split('_')[:-1])\n",
    "                    # prepend underscore to extension\n",
    "                    extension = '_' + extension\n",
    "\n",
    "                # read the stats file, name columns: 'name', 'value', 'description'\n",
    "                df = pd.read_csv(os.path.join(resultsdir, c, w, stat_file), header=None).T\n",
    "                df.columns = df.iloc[0]\n",
    "                df.drop(0,inplace=True)\n",
    "                # add a new column called 'config' with the config name\n",
    "                df['config'] = c + extension\n",
    "                # add a new column called 'workload' with the workload name\n",
    "                df['workload'] = w\n",
    "                # print the stats file\n",
    "                # print('Config: {}, Workload: {}, Stats: {}'.format(c, w, df))\n",
    "                # append the stats to the list\n",
    "                df.reset_index(inplace=True, drop=True)\n",
    "                stats_per_config_workload.append(df)\n",
    "        else:\n",
    "            print('Config: {}, Workload: {}, Stats: No stats file found'.format(c, w))\n",
    "\n",
    "# concatenate all stats into one dataframe\n",
    "stats = pd.concat(stats_per_config_workload)\n",
    "\n",
    "# find elements where workload does not contain '-'\n",
    "# these are multi core workloads\n",
    "stats = stats[~stats['workload'].str.contains('-')]\n",
    "\n",
    "# remove these two workloads: stream_10.trace and random_10.trace\n",
    "stats = stats[~stats['workload'].isin(['gups'])]\n",
    "# also from workloads\n",
    "workloads = [w for w in workloads if not w in ['gups']]\n",
    "\n",
    "# make sure config is string\n",
    "stats['config'] = stats['config'].astype(str)\n",
    "\n",
    "# remove \"-16DR\" from config names\n",
    "stats['config'] = stats['config'].str.replace('-16DR', '')\n",
    "\n",
    "# replace 1K with 1000 in config names\n",
    "stats['config'] = stats['config'].str.replace('1K', '1000')\n",
    "\n",
    "# replace 'Baseline' with 'Baseline0'\n",
    "stats['config'] = stats['config'].str.replace('Baseline', 'Baseline0')\n",
    "\n",
    "# add a new column that stores in integer the number in the config name\n",
    "stats['nrh'] = stats['config'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# remove numbers from config names\n",
    "stats['config'] = stats['config'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# remove yaml from config names\n",
    "stats['config'] = stats['config'].str.replace('.yaml', '')\n",
    "\n",
    "# replace SPR with APAR\n",
    "stats['config'] = stats['config'].str.replace('ABACUS', MY_MECHANISM_NAME)\n",
    "\n",
    "stats.loc[stats['workload'] == 'random_10.trace', 'workload'] = 'gups'\n",
    "\n",
    "# increasing order of rbmpki\n",
    "# order = ['511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', '500.perlbench', '523.xalancbmk', '510.parest', '557.xz', '482.sphinx3', '505.mcf', '436.cactusADM', '471.omnetpp', '473.astar', '483.xalancbmk', '462.libquantum', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf']\n",
    "order = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "# remove all workloads not in order\n",
    "stats = stats[stats['workload'].isin(order)]\n",
    "# also from the workload list\n",
    "workloads = [w for w in workloads if w in order]\n",
    "\n",
    "# order workloads according to the order\n",
    "stats['workload'] = pd.Categorical(stats['workload'], categories=order, ordered=True)\n",
    "\n",
    "stats_copy = stats.copy()\n",
    "\n",
    "# import MultipleLocator\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=5)\n",
    "\n",
    "stats = stats_copy.copy()\n",
    "\n",
    "# instructions per cycle (IPC) is record_cycles_insts_0 / record_cycs_core_0\n",
    "stats['ramulator.ipc'] = stats['ramulator.record_insts_core_0'] / stats['ramulator.record_cycs_core_0']\n",
    "\n",
    "\n",
    "stats['ramulator.rbmpki'] = (stats['ramulator.row_conflicts_channel_0_core'] + stats['ramulator.row_misses_channel_0_core']) /\\\n",
    "                            stats['ramulator.record_insts_core_0'] * 1000\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline = stats[stats['config'] == 'Baseline']\n",
    "baseline = baseline[['workload', 'ramulator.ipc', 'ramulator.read_latency_avg_0', 'ramulator.rbmpki', 'ramulator.window_full_stall_cycles_core_0']]\n",
    "# baseline\n",
    "baseline.columns = ['workload', 'ramulator.baseline_ipc', 'ramulator.baseline_read_latency_avg_0', 'ramulator.baseline_rbmpki', 'ramulator.baseline_stall_cycles']\n",
    "stats = pd.merge(stats, baseline, on='workload')\n",
    "#hydra baseline\n",
    "hydra_baseline = stats[stats['config'] == 'Hydra-Baseline']\n",
    "hydra_baseline = hydra_baseline[['workload', 'ramulator.ipc']]\n",
    "# hydra_baseline\n",
    "hydra_baseline.columns = ['workload', 'ramulator.hydra_baseline_ipc']\n",
    "stats = pd.merge(stats, hydra_baseline, on='workload')\n",
    "\n",
    "stats['ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc']\n",
    "stats['ramulator.normalized_read_latency'] = stats['ramulator.read_latency_avg_0'] / stats['ramulator.baseline_read_latency_avg_0']\n",
    "stats['ramulator.normalized_stall_cycles'] = stats['ramulator.window_full_stall_cycles_core_0'] / stats['ramulator.baseline_stall_cycles']\n",
    "stats['ramulator.normalized_rbmpki'] = stats['ramulator.rbmpki'] / stats['ramulator.baseline_rbmpki']\n",
    "\n",
    "# normalized ipc for hydra is not correct, so we overwrite it with the correct value\n",
    "stats.loc[stats['config'].str.contains('Hydra'), 'ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.hydra_baseline_ipc']\n",
    "\n",
    "# add the geometric normalized ipc average as a new workload to every config\n",
    "geometric_mean = stats.groupby(['config','nrh'])['ramulator.normalized_ipc'].apply(lambda x: x.prod()**(1.0/len(x))).reset_index()\n",
    "geometric_mean['workload'] = 'GeoMean'\n",
    "\n",
    "\n",
    "\n",
    "stats = pd.concat([stats, geometric_mean])\n",
    "\n",
    "# order in decreasing nRH (the nRH column) use pd.Categorical on nrh\n",
    "stats['nrh'] = pd.Categorical(stats['nrh'], categories=[1000, 500, 250, 125], ordered=True)\n",
    "\n",
    "\n",
    "# order = ['GeoMean', '531.deepsjeng', '502.gcc', '541.leela', '435.gromacs', '481.wrf', '458.sjeng', '445.gobmk', '444.namd', '508.namd', '401.bzip2', '456.hmmer', '403.gcc', '464.h264ref', '526.blender', '447.dealII', '544.nab', '523.xalancbmk', '500.perlbench', '538.imagick', '525.x264', '507.cactuBSSN', '511.povray', '462.libquantum', '473.astar', '510.parest', '482.sphinx3', '505.mcf', '557.xz', '471.omnetpp', '483.xalancbmk', '436.cactusADM', '520.omnetpp', '450.soplex', '470.lbm', '519.lbm', '434.zeusmp', '433.milc', '459.GemsFDTD', '549.fotonik3d', '429.mcf', '437.leslie3d']\n",
    "order = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp','GeoMean']\n",
    "\n",
    "# order = ['GeoMean', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', '500.perlbench', '523.xalancbmk', '510.parest', '557.xz', '482.sphinx3', '505.mcf', '436.cactusADM', '471.omnetpp', '473.astar', '483.xalancbmk', '462.libquantum', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf']\n",
    "\n",
    "stats['workload'] = pd.Categorical(stats['workload'], categories=order, ordered=True)\n",
    "\n",
    "#barplot of normalized IPC, also draw edges around bars\n",
    "fig, ax = plt.subplots(figsize=(13, 4))\n",
    "ax = sns.barplot(x='workload', y='ramulator.normalized_ipc', hue='nrh', data=stats[(stats['config'] == 'ABACuS')], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "\n",
    "ax.set_xlabel('Workload')\n",
    "ax.set_ylabel('Normalized IPC')\n",
    "# move ylabel down\n",
    "ax.yaxis.set_label_coords(-0.045,0.45)\n",
    "# draw a red line at y = 1.0, label it as baseline IPC\n",
    "ax.axhline(y=1.0, color='r', linestyle='--')\n",
    "# write above the red line 'baseline IPC' using the same pastel red color\n",
    "ax.text(0.01, 0.7, 'Baseline IPC', color='#e74c3c', transform=ax.transAxes, fontsize=15)\n",
    "# extend the y axis to 1.2\n",
    "ax.set_ylim(0.6, 1.2)\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[2].set_color('#e74c3c')\n",
    "# rotate x axis ticks\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "ax.tick_params(axis='y', which='major', labelsize=12)\n",
    "\n",
    "# # draw vertical lines to separate the rowhammer threshold values\n",
    "ax.axvline(x=26.52, color='grey', linestyle='-', alpha=0.5)\n",
    "# put text before the line saying \"LOW RBMPKI\"\n",
    "ax.text(0.33, 0.7, 'LOW RBMPKI', color='grey', transform=ax.transAxes, fontsize=12)\n",
    "# put arrow to the left above text\n",
    "ax.annotate('', xy=(19.5, 1.08), xytext=(25.5, 1.08), arrowprops=dict(facecolor='grey', shrink=0.01, width=3, headwidth=10, alpha=0.5))\n",
    "ax.axvline(x=46.52, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.text(0.65, 0.7, 'MED. RBMPKI', color='grey', transform=ax.transAxes, fontsize=12)\n",
    "ax.annotate('', xy=(39.5, 1.08), xytext=(45.5, 1.08), arrowprops=dict(facecolor='grey', shrink=0.01, width=3, headwidth=10, alpha=0.5))\n",
    "ax.axvline(x=61.52, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.text(0.875, 0.7, 'HIGH RBMPKI', color='grey', transform=ax.transAxes, fontsize=12)\n",
    "ax.annotate('', xy=(53.5, 1.08), xytext=(60.5, 1.08), arrowprops=dict(facecolor='grey', shrink=0.01, width=3, headwidth=10, alpha=0.5))\n",
    "\n",
    "# make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(16)\n",
    "#ax.yaxis.label.set_fontweight('bold')\n",
    "ax.yaxis.label.set_size(16)\n",
    "#ax.xaxis.label.set_fontweight('bold')\n",
    "\n",
    "# put the legend on top of the plot\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4, fancybox=True, shadow=True, fontsize=12)\n",
    "# prepend \"nRH\" to legend names\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, ['$N_{RH}$ = ' + label for label in labels], loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4, fancybox=True, shadow=True, fontsize=12)\n",
    "\n",
    "# highlight the geometric mean ax label\n",
    "ax.get_xticklabels()[62].set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_performance_single_core.pdf', bbox_inches='tight')\n",
    "# export data to csv\n",
    "stats.to_csv('abacus_performance_single_core.csv', index=False)\n",
    "\n",
    "# numbers to put in paper\n",
    "\n",
    "# ABACuS at 1000 nRH normalized_ipc for geomean workload\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'])\n",
    "# ABACuS at 1000 nRH minimum normalized_ipc\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000)]['ramulator.normalized_ipc'].min())\n",
    "# ABACuS at 1000 nRH preventive_refreshes_channel_0_core average\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000)]['ramulator.preventive_refreshes_channel_0_core'].mean())\n",
    "# ABACuS at 1000 nRH normalized_read_latency average\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000)]['ramulator.normalized_read_latency'].mean())\n",
    "\n",
    "# above stats for nrh = 125\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'])\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_ipc'].min())\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.preventive_refreshes_channel_0_core'].mean())\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_read_latency'].mean())\n",
    "\n",
    "# REGA at 125 nrh normalized_ipc for geomean workload\n",
    "print(stats[(stats['config'] == 'REGA') & (stats['nrh'] == 125) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'])\n",
    "\n",
    "# PARA at 125 nrh normalized_ipc for geomean workload\n",
    "print(stats[(stats['config'] == 'PARA') & (stats['nrh'] == 125) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'])\n",
    "\n",
    "# PARA at 1000 nrh normalized_ipc for geomean workload\n",
    "print(stats[(stats['config'] == 'PARA') & (stats['nrh'] == 1000) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'])\n",
    "\n",
    "# Hydra at 125 nrh normalized_ipc for geomean workload divided by Abacus at 125 nrh normalized_ipc for geomean workload\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0]/ stats[(stats['config'] == 'Hydra') & (stats['nrh'] == 125) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0])\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 250) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0]/ stats[(stats['config'] == 'Hydra') & (stats['nrh'] == 250) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0])\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 500) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0]/ stats[(stats['config'] == 'Hydra') & (stats['nrh'] == 500) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0])\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0]/ stats[(stats['config'] == 'Hydra') & (stats['nrh'] == 1000) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0])\n",
    "\n",
    "\n",
    "# Hydra normalized RBMPKI at nrh = 125 for geomean\n",
    "print('hydravsabacus rbmpki: ', stats[(stats['config'] == 'Hydra') & (stats['nrh'] == 125)]['ramulator.normalized_rbmpki'].mean()- stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_rbmpki'].mean())\n",
    "# Hydra at 125 nRH normalized_read_latency average\n",
    "print('hydravsabacus latency: ', stats[(stats['config'] == 'Hydra') & (stats['nrh'] == 125)]['ramulator.normalized_read_latency'].mean() - stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_read_latency'].mean())\n",
    "\n",
    "# mean number of preventive refreshes for ABACuS at 125 nrh\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.preventive_refreshes_channel_0_core'].mean())\n",
    "print(stats[(stats['config'] == 'Graphene') & (stats['nrh'] == 125)]['ramulator.preventive_refreshes_channel_0_core'].mean())\n",
    "\n",
    "# mean normalized_stall_cycles for ABACuS at 125 nrh\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_stall_cycles'].mean())\n",
    "print(stats[(stats['config'] == 'Graphene') & (stats['nrh'] == 125)]['ramulator.normalized_stall_cycles'].mean())\n",
    "\n",
    "evaluation_paragraph = \"\"\"\n",
    "ABACuS average slowdown at 1K nRH {avg_slowdown_1k}\n",
    "ABACuS max slowdown at 1K nRH {max_slowdown_1k}\n",
    "ABACuS average memory latency increase at 1K nRH {avg_mem_lat_1k}\n",
    "\n",
    "ABACuS average slowdown at 125 nRH {avg_slowdown_125}\n",
    "ABACuS max slowdown at 125 nRH {max_slowdown_125}\n",
    "ABACuS average memory latency increase at 125 nRH {avg_mem_lat_125}\n",
    "\"\"\".format(\n",
    "    avg_slowdown_1k=1-(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000)]['ramulator.normalized_ipc'].mean()),\n",
    "    max_slowdown_1k=1-(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000)]['ramulator.normalized_ipc'].min()),\n",
    "    avg_mem_lat_1k=stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000)]['ramulator.normalized_read_latency'].mean(),\n",
    "    avg_slowdown_125=1-(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_ipc'].mean()),\n",
    "    max_slowdown_125=1-(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_ipc'].min()),\n",
    "    avg_mem_lat_125=stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_read_latency'].mean(),\n",
    ")\n",
    "\n",
    "print(evaluation_paragraph)\n",
    "\n",
    "\n",
    "# Graphene at 125 nrh normalized_ipc for geomean workload divided by Abacus at 125 nrh normalized_ipc for geomean workload\n",
    "# print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0]/ stats[(stats['config'] == 'Graphene') & (stats['nrh'] == 125) & (stats['workload'] == 'GeoMean')]['ramulator.normalized_ipc'].values[0])\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=1)\n",
    "\n",
    "stats = stats_copy.copy()\n",
    "\n",
    "# instructions per cycle (IPC) is record_cycles_insts_0 / record_cycs_core_0\n",
    "stats['ramulator.ipc'] = stats['ramulator.record_insts_core_0'] / stats['ramulator.record_cycs_core_0']\n",
    "\n",
    "\n",
    "stats['ramulator.rbmpki'] = (stats['ramulator.row_conflicts_channel_0_core'] + stats['ramulator.row_misses_channel_0_core']) /\\\n",
    "                            stats['ramulator.record_insts_core_0'] * 1000\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline = stats[stats['config'] == 'Baseline']\n",
    "baseline = baseline[['workload', 'ramulator.ipc', 'ramulator.read_latency_avg_0', 'ramulator.rbmpki', 'ramulator.window_full_stall_cycles_core_0']]\n",
    "# baseline\n",
    "baseline.columns = ['workload', 'ramulator.baseline_ipc', 'ramulator.baseline_read_latency_avg_0', 'ramulator.baseline_rbmpki', 'ramulator.baseline_stall_cycles']\n",
    "stats = pd.merge(stats, baseline, on='workload')\n",
    "#hydra baseline\n",
    "hydra_baseline = stats[stats['config'] == 'Hydra-Baseline']\n",
    "hydra_baseline = hydra_baseline[['workload', 'ramulator.ipc']]\n",
    "# hydra_baseline\n",
    "hydra_baseline.columns = ['workload', 'ramulator.hydra_baseline_ipc']\n",
    "stats = pd.merge(stats, hydra_baseline, on='workload')\n",
    "\n",
    "# order and sort nRH high to low\n",
    "stats['nrh'] = pd.Categorical(stats['nrh'], categories=[1000, 500, 250, 125], ordered=True)\n",
    "stats = stats.sort_values('nrh')\n",
    "\n",
    "stats['ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc']\n",
    "stats['ramulator.normalized_read_latency'] = stats['ramulator.read_latency_avg_0'] / stats['ramulator.baseline_read_latency_avg_0']\n",
    "stats['ramulator.normalized_stall_cycles'] = stats['ramulator.window_full_stall_cycles_core_0'] / stats['ramulator.baseline_stall_cycles']\n",
    "stats['ramulator.normalized_rbmpki'] = stats['ramulator.rbmpki'] / stats['ramulator.baseline_rbmpki']\n",
    "\n",
    "# normalized ipc for hydra is not correct, so we overwrite it with the correct value\n",
    "stats.loc[stats['config'].str.contains('Hydra'), 'ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.hydra_baseline_ipc']\n",
    "\n",
    "# add the geometric normalized ipc average as a new workload to every config\n",
    "# geometric_mean = stats.groupby(['config','nrh'])['ramulator.normalized_ipc'].apply(lambda x: x.prod()**(1.0/len(x))).reset_index()\n",
    "# geometric_mean['workload'] = 'GeoMean'\n",
    "\n",
    "# stats = pd.concat([stats, geometric_mean])\n",
    "\n",
    "# # order = ['GeoMean', '531.deepsjeng', '502.gcc', '541.leela', '435.gromacs', '481.wrf', '458.sjeng', '445.gobmk', '444.namd', '508.namd', '401.bzip2', '456.hmmer', '403.gcc', '464.h264ref', '526.blender', '447.dealII', '544.nab', '523.xalancbmk', '500.perlbench', '538.imagick', '525.x264', '507.cactuBSSN', '511.povray', '462.libquantum', '473.astar', '510.parest', '482.sphinx3', '505.mcf', '557.xz', '471.omnetpp', '483.xalancbmk', '436.cactusADM', '520.omnetpp', '450.soplex', '470.lbm', '519.lbm', '434.zeusmp', '433.milc', '459.GemsFDTD', '549.fotonik3d', '429.mcf', '437.leslie3d']\n",
    "# order = ['GeoMean', 'h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "low_rbmpki = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver']\n",
    "med_rbmpki = ['ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2']\n",
    "high_rbmpki = ['433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "# add new column called rbmpki_category, set category if workload matches one of the above lists\n",
    "stats['rbmpki_category'] = np.where(stats['workload'].isin(low_rbmpki), 'low (<2)', np.where(stats['workload'].isin(med_rbmpki), 'medium (<10)', np.where(stats['workload'].isin(high_rbmpki), 'high (>10)', 'none')))\n",
    "\n",
    "# geometric_mean = stats.groupby(['config','nrh','rbmpki_category'])['ramulator.normalized_ipc'].apply(lambda x: x.prod()**(1.0/len(x))).reset_index()\n",
    "# geometric_mean['workload'] = 'GeoMean'\n",
    "# stats = pd.concat([stats, geometric_mean])\n",
    "\n",
    "# remove rows where rbmpki_category is none\n",
    "stats = stats[stats['rbmpki_category'] != 'none']\n",
    "\n",
    "order = ['low (<2)', 'medium (<10)', 'high (>10)']\n",
    "stats['rbmpki_category'] = pd.Categorical(stats['rbmpki_category'], categories=order, ordered=True)\n",
    "\n",
    "#barplot of normalized IPC, also draw edges around bars\n",
    "fig, ax = plt.subplots(figsize=(7, 2))\n",
    "ax = sns.boxplot(x='rbmpki_category', y='ramulator.normalized_ipc', hue='nrh', data=stats[(stats['config'] == 'ABACuS')], showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\"})\n",
    "\n",
    "ax.set_xlabel('Row buffer misses per kilo instructions (RBMPKI)')\n",
    "ax.set_ylabel('Normalized IPC\\ndistribution')\n",
    "ax.axhline(y=1.0, color='#e74c3c', linestyle='--')\n",
    "# write above the red line 'baseline IPC' using the same pastel red color\n",
    "ax.text(0.01, 0.85, 'Baseline IPC', color='#e74c3c', transform=ax.transAxes, fontsize=15)\n",
    "# extend the y axis to 1.2\n",
    "ax.set_ylim(0.6, 1.1)\n",
    "# rotate x axis ticks\n",
    "ax.set_xticklabels(ax.get_xticklabels())\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "ax.tick_params(axis='y', which='major', labelsize=12)\n",
    "\n",
    "# have four y axis ticks\n",
    "ax.set_yticks([0.6, 0.7, 0.8, 0.9, 1.0, 1.1])\n",
    "\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[4].set_color('#e74c3c')\n",
    "# # make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(16)\n",
    "# ax.yaxis.label.set_fontweight('bold')\n",
    "ax.yaxis.label.set_size(16)\n",
    "# ax.xaxis.label.set_fontweight('bold')\n",
    "\n",
    "# draw a circle around the minimum value of abacus at nrh = 1000\n",
    "# get the minimum value of abacus at nrh = 1000\n",
    "ax.add_patch(plt.Circle((2.3, 0.87), 0.03, color='blue', fill=False, clip_on=False))\n",
    "ax.add_patch(plt.Circle((2.1, 0.75), 0.03, color='blue', fill=False, clip_on=False))\n",
    "ax.add_patch(plt.Circle((1.9, 0.725), 0.03, color='blue', fill=False, clip_on=False))\n",
    "ax.add_patch(plt.Circle((1.7, 0.677), 0.03, color='blue', fill=False, clip_on=False))\n",
    "# write the minimum value of abacus at nrh = 1000\n",
    "ax.text(1.59, 0.8, 'gups', color='blue', fontsize=12, bbox=dict(facecolor='white', edgecolor='blue', boxstyle='round,pad=0.2', alpha = 0.95))\n",
    "# draw an arrow from text to circle\n",
    "# ax.annotate(\"\", xy=(-0.32, 0.88), xytext=(-0.2, 0.47), arrowprops=dict(arrowstyle=\"->\", color='blue', lw=1))\n",
    "\n",
    "\n",
    "# # put the legend on top of the plot\n",
    "# ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4, fancybox=True, shadow=True, fontsize=10)\n",
    "# # prepend \"nRH\" to legend names\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "newlabels = ['$N_{RH}$ = ' + label for label in labels]\n",
    "newlabels[0] += ' (leftmost box)'\n",
    "newlabels[3] += ' (rightmost box)'\n",
    "ax.legend(handles, newlabels, loc='upper center', bbox_to_anchor=(0.235, 0.74), ncol=1, fancybox=True, shadow=True, fontsize=10)\n",
    "\n",
    "# remove legend\n",
    "ax.legend_.remove()\n",
    "\n",
    "ax.annotate('$N_{RH}=1000$ (leftmost box)\\n$N_{RH}=500$\\n$N_{RH}=250$\\n$N_{RH}=125$ (rightmost box)', xy=(0.05, 0.60), xycoords='axes fraction',\n",
    "            size=10, ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', fc='w', ec='k'))\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_performance_single_core_small.pdf', bbox_inches='tight')\n",
    "# export data to csv\n",
    "stats.to_csv('abacus_performance_single_core_small.csv', index=False)\n",
    "\n",
    "# ABACuS average normalized IPC at nRH = 1000\n",
    "print(1-stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000)]['ramulator.normalized_ipc'].mean())\n",
    "# ABACuS min. normalized IPC at nRH = 1000\n",
    "print(1-stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000)]['ramulator.normalized_ipc'].min())\n",
    "# ABACuS average normalized memory latency at nRH = 1000\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 1000)]['ramulator.normalized_read_latency'].mean())\n",
    "\n",
    "# ABACuS average normalized IPC at nRH = 125\n",
    "print(1-stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_ipc'].mean())\n",
    "# ABACuS min. normalized IPC at nRH = 125\n",
    "print(1-stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_ipc'].min())\n",
    "# ABACuS average normalized memory latency at nRH = 125\n",
    "print(stats[(stats['config'] == 'ABACuS') & (stats['nrh'] == 125)]['ramulator.normalized_read_latency'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figures 9 and 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MECHANISM_NAME = 'ABACuS'\n",
    "\n",
    "### READ RESULTS INTO PANDAS DATAFRAME\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsdir = \"../results\"\n",
    "# list all directories in resultsdir\n",
    "configs = [d for d in os.listdir(resultsdir) if os.path.isdir(os.path.join(resultsdir, d))]\n",
    "configs = ['Baseline.yaml', 'Hydra-Baseline.yaml', 'REGA125.yaml', 'REGA250.yaml', 'REGA500.yaml', 'REGA1000.yaml', 'Graphene125.yaml', 'Graphene250.yaml', 'Graphene500.yaml', 'Graphene1000.yaml', 'Hydra125.yaml', 'Hydra250.yaml', 'Hydra500.yaml', 'Hydra1000.yaml', 'PARA125.yaml', 'PARA250.yaml', 'PARA500.yaml', 'PARA1000.yaml', 'ABACUS125.yaml', 'ABACUS250.yaml', 'ABACUS500.yaml', 'ABACUS1000.yaml']\n",
    "\n",
    "# print found configs\n",
    "print('Found configs: {}'.format(configs))\n",
    "# list all directories under all configs\n",
    "workloads = []\n",
    "for c in configs:\n",
    "    workloads.append([d for d in os.listdir(os.path.join(resultsdir, c)) if os.path.isdir(os.path.join(resultsdir, c, d))])\n",
    "# find only the intersection of all workloads\n",
    "workloads = list(set.intersection(*map(set, workloads)))\n",
    "# print found workloads\n",
    "print('Found workloads: {}'.format(workloads))\n",
    "\n",
    "stats_per_config_workload = []\n",
    "workloads = [w for w in workloads if not '-' in w]\n",
    "\n",
    "# for every config + workload directory\n",
    "for c in configs:\n",
    "    for w in workloads:\n",
    "        # find all files in the directory\n",
    "        files = [f for f in os.listdir(os.path.join(resultsdir, c, w)) if os.path.isfile(os.path.join(resultsdir, c, w, f))]\n",
    "        # find the stats file\n",
    "        stat_files = [f for f in files if f.endswith('output.txt')]\n",
    "        # if there is a stats file\n",
    "        if stat_files:\n",
    "            for stat_file in stat_files:\n",
    "                # if the stats_file has less than three lines skip it\n",
    "                if len(open(os.path.join(resultsdir, c, w, stat_file)).readlines()) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # print the name of the stats_file\n",
    "                print('Found stats file: {}'.format(os.path.join(os.path.join(resultsdir, c, w, stat_file))))\n",
    "\n",
    "                lines = open(os.path.join(resultsdir, c, w, stat_file)).readlines()\n",
    "                total_energy = 0\n",
    "                for l in lines:\n",
    "                    # if line contains nJ, add l.split()[-2] to total_energy\n",
    "                    if 'Total Idle energy:' in l:\n",
    "                        continue\n",
    "                    if 'nJ' in l:\n",
    "                        total_energy += float(l.split()[-2])\n",
    "                    if l.startswith('REF CMD energy'):\n",
    "                        break\n",
    "\n",
    "                \n",
    "                # create a df with the config, workload and total_energy\n",
    "                df = pd.DataFrame({'config': [c], 'workload': [w], 'total_energy': [total_energy]})\n",
    "                df.reset_index(inplace=True, drop=True)\n",
    "                stats_per_config_workload.append(df)\n",
    "        else:\n",
    "            print('Config: {}, Workload: {}, Stats: No stats file found'.format(c, w))\n",
    "\n",
    "# concatenate all stats into one dataframe\n",
    "stats = pd.concat(stats_per_config_workload)\n",
    "\n",
    "# find elements where workload does not contain '-'\n",
    "# these are multi core workloads\n",
    "stats = stats[~stats['workload'].str.contains('-')]\n",
    "\n",
    "# remove these two workloads: stream_10.trace and random_10.trace\n",
    "stats = stats[~stats['workload'].isin(['gups'])]\n",
    "# also from workloads\n",
    "workloads = [w for w in workloads if not w in ['gups']]\n",
    "\n",
    "# remove \"-16DR\" from config names\n",
    "stats['config'] = stats['config'].str.replace('-16DR', '')\n",
    "\n",
    "# replace 1K with 1000 in config names\n",
    "stats['config'] = stats['config'].str.replace('1K', '1000')\n",
    "\n",
    "# replace 'Baseline' with 'Baseline0'\n",
    "stats['config'] = stats['config'].str.replace('Baseline', 'Baseline0')\n",
    "\n",
    "# add a new column that stores in integer the number in the config name\n",
    "stats['nrh'] = stats['config'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# remove numbers from config names\n",
    "stats['config'] = stats['config'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# remove yaml from config names\n",
    "stats['config'] = stats['config'].str.replace('.yaml', '')\n",
    "\n",
    "# replace SPR with APAR\n",
    "stats['config'] = stats['config'].str.replace('ABACUS', MY_MECHANISM_NAME)\n",
    "\n",
    "stats.loc[stats['workload'] == 'random_10.trace', 'workload'] = 'gups'\n",
    "\n",
    "# increasing order of rbmpki\n",
    "#order = ['511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', '500.perlbench', '523.xalancbmk', '510.parest', '557.xz', '482.sphinx3', '505.mcf', '436.cactusADM', '471.omnetpp', '473.astar', '483.xalancbmk', '462.libquantum', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf']\n",
    "order = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "\n",
    "# remove all workloads not in order\n",
    "stats = stats[stats['workload'].isin(order)]\n",
    "# also from the workload list\n",
    "workloads = [w for w in workloads if w in order]\n",
    "\n",
    "\n",
    "# order workloads according to the order\n",
    "stats['workload'] = pd.Categorical(stats['workload'], categories=order, ordered=True)\n",
    "\n",
    "stats_copy = stats.copy()\n",
    "\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=1)\n",
    "\n",
    "stats = stats_copy.copy()\n",
    "\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline = stats[stats['config'] == 'Baseline']\n",
    "baseline = baseline[['workload', 'total_energy']]\n",
    "# baseline\n",
    "baseline.columns = ['workload', 'baseline_energy']\n",
    "stats = pd.merge(stats, baseline, on='workload')\n",
    "\n",
    "#hydra baseline\n",
    "hydra_baseline = stats[stats['config'] == 'Hydra-Baseline']\n",
    "hydra_baseline = hydra_baseline[['workload', 'total_energy']]\n",
    "# hydra_baseline\n",
    "hydra_baseline.columns = ['workload', 'ramulator.hydra_baseline_energy']\n",
    "stats = pd.merge(stats, hydra_baseline, on='workload')\n",
    "\n",
    "stats['normalized_energy'] = stats['total_energy'] / stats['baseline_energy']\n",
    "\n",
    "# normalized ipc for hydra is not correct, so we overwrite it with the correct value\n",
    "stats.loc[stats['config'].str.contains('Hydra'), 'normalized_energy'] = stats['total_energy'] / stats['ramulator.hydra_baseline_energy']\n",
    "\n",
    "# add the geometric normalized ipc average as a new workload to every config\n",
    "# geometric_mean = stats.groupby(['config','nrh'])['ramulator.normalized_ipc'].apply(lambda x: x.prod()**(1.0/len(x))).reset_index()\n",
    "# geometric_mean['workload'] = 'GeoMean'\n",
    "\n",
    "# stats = pd.concat([stats, geometric_mean])\n",
    "\n",
    "# # order = ['GeoMean', '531.deepsjeng', '502.gcc', '541.leela', '435.gromacs', '481.wrf', '458.sjeng', '445.gobmk', '444.namd', '508.namd', '401.bzip2', '456.hmmer', '403.gcc', '464.h264ref', '526.blender', '447.dealII', '544.nab', '523.xalancbmk', '500.perlbench', '538.imagick', '525.x264', '507.cactuBSSN', '511.povray', '462.libquantum', '473.astar', '510.parest', '482.sphinx3', '505.mcf', '557.xz', '471.omnetpp', '483.xalancbmk', '436.cactusADM', '520.omnetpp', '450.soplex', '470.lbm', '519.lbm', '434.zeusmp', '433.milc', '459.GemsFDTD', '549.fotonik3d', '429.mcf', '437.leslie3d']\n",
    "# order = ['GeoMean', 'h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "low_rbmpki = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver']\n",
    "med_rbmpki = ['ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2']\n",
    "high_rbmpki = ['433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "# add new column called rbmpki_category, set category if workload matches one of the above lists\n",
    "stats['rbmpki_category'] = np.where(stats['workload'].isin(low_rbmpki), 'low (<2)', np.where(stats['workload'].isin(med_rbmpki), 'medium (<10)', np.where(stats['workload'].isin(high_rbmpki), 'high (>10)', 'none')))\n",
    "\n",
    "# geometric_mean = stats.groupby(['config','nrh','rbmpki_category'])['ramulator.normalized_ipc'].apply(lambda x: x.prod()**(1.0/len(x))).reset_index()\n",
    "# geometric_mean['workload'] = 'GeoMean'\n",
    "# stats = pd.concat([stats, geometric_mean])\n",
    "\n",
    "# order and sort nRH high to low\n",
    "stats['nrh'] = pd.Categorical(stats['nrh'], categories=[1000, 500, 250, 125], ordered=True)\n",
    "stats = stats.sort_values('nrh')\n",
    "\n",
    "# remove rows where rbmpki_category is none\n",
    "stats = stats[stats['rbmpki_category'] != 'none']\n",
    "\n",
    "order = ['low (<2)', 'medium (<10)', 'high (>10)']\n",
    "stats['rbmpki_category'] = pd.Categorical(stats['rbmpki_category'], categories=order, ordered=True)\n",
    "\n",
    "#barplot of normalized IPC, also draw edges around bars\n",
    "fig, ax = plt.subplots(figsize=(7, 2))\n",
    "ax = sns.boxplot(x='rbmpki_category', y='normalized_energy', hue='nrh', data=stats[(stats['config'] == 'ABACuS')], showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\"})\n",
    "\n",
    "ax.set_xlabel('Row buffer misses per kilo instructions (RBMPKI)')\n",
    "ax.set_ylabel('Normalized\\nDRAM energy\\ndistribution')\n",
    "ax.axhline(y=1.0, color='#e74c3c', linestyle='--')\n",
    "# write above the red line 'baseline IPC' using the same pastel red color\n",
    "ax.text(0.01, 0.15, 'Baseline DRAM energy', color='#e74c3c', transform=ax.transAxes, fontsize=15)\n",
    "# extend the y axis to 1.2\n",
    "# ax.set_ylim(0.8, 1.05)\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[1].set_color('#e74c3c')\n",
    "# rotate x axis ticks\n",
    "ax.set_xticklabels(ax.get_xticklabels())\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "ax.tick_params(axis='y', which='major', labelsize=12)\n",
    "\n",
    "# have four y axis ticks\n",
    "# ax.set_yticks([0.8, 0.85, 0.9, 0.95, 1.0, 1.05])\n",
    "\n",
    "# # make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(16)\n",
    "# ax.yaxis.label.set_fontweight('bold')\n",
    "ax.yaxis.label.set_size(16)\n",
    "# ax.xaxis.label.set_fontweight('bold')\n",
    "ax.add_patch(plt.Circle((2.303, 1.30), 0.05, color='blue', fill=False, clip_on=False))\n",
    "ax.add_patch(plt.Circle((2.103, 1.71), 0.05, color='blue', fill=False, clip_on=False))\n",
    "ax.add_patch(plt.Circle((1.903, 1.81), 0.05, color='blue', fill=False, clip_on=False))\n",
    "ax.add_patch(plt.Circle((1.7, 2.01), 0.05, color='blue', fill=False, clip_on=False))\n",
    "# write the minimum value of abacus at nrh = 1000\n",
    "ax.text(1.55, 1.5, 'gups', color='blue', fontsize=12, bbox=dict(facecolor='white', edgecolor='blue', boxstyle='round,pad=0.2', alpha = 0.95))\n",
    "\n",
    "# # put the legend on top of the plot\n",
    "# ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4, fancybox=True, shadow=True, fontsize=10)\n",
    "# # prepend \"nRH\" to legend names\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, ['$N_{RH}$ = ' + label for label in labels], loc='upper center', bbox_to_anchor=(0.15, 1.0), ncol=1, fancybox=True, shadow=True, fontsize=10)\n",
    "\n",
    "# remove legend\n",
    "ax.legend_.remove()\n",
    "\n",
    "ax.annotate('$N_{RH}=1000$ (leftmost box)\\n$N_{RH}=500$\\n$N_{RH}=250$\\n$N_{RH}=125$ (rightmost box)', xy=(0.05, 0.90), xycoords='axes fraction',\n",
    "            size=10, ha='left', va='top',\n",
    "            bbox=dict(boxstyle='round', fc='w', ec='k'))\n",
    "\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_energy_single_core_small.pdf', bbox_inches='tight')\n",
    "# export data to csv\n",
    "stats.to_csv('abacus_energy_single_core_small.csv', index=False)\n",
    "\n",
    "\n",
    "# ABACuS average normalized energy at 1K nRH\n",
    "print(stats[(stats['nrh'] == 1000) & (stats['config'] == 'ABACuS')]['normalized_energy'].mean())\n",
    "# ABACuS max. normalized energy at 1K nRH\n",
    "print(stats[(stats['nrh'] == 1000) & (stats['config'] == 'ABACuS')]['normalized_energy'].max())\n",
    "# ABACuS average normalized energy at 125 nRH\n",
    "print(stats[(stats['nrh'] == 125) & (stats['config'] == 'ABACuS')]['normalized_energy'].mean())\n",
    "# ABACuS max. normalized energy at 125 nRH\n",
    "print(stats[(stats['nrh'] == 125) & (stats['config'] == 'ABACuS')]['normalized_energy'].max())\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=5)\n",
    "\n",
    "stats = stats_copy.copy()\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline = stats[stats['config'] == 'Baseline']\n",
    "baseline = baseline[['workload', 'total_energy']]\n",
    "# baseline\n",
    "baseline.columns = ['workload', 'baseline_energy']\n",
    "stats = pd.merge(stats, baseline, on='workload')\n",
    "\n",
    "#hydra baseline\n",
    "hydra_baseline = stats[stats['config'] == 'Hydra-Baseline']\n",
    "hydra_baseline = hydra_baseline[['workload', 'total_energy']]\n",
    "# hydra_baseline\n",
    "hydra_baseline.columns = ['workload', 'ramulator.hydra_baseline_energy']\n",
    "stats = pd.merge(stats, hydra_baseline, on='workload')\n",
    "\n",
    "stats['normalized_energy'] = stats['total_energy'] / stats['baseline_energy']\n",
    "\n",
    "# normalized ipc for hydra is not correct, so we overwrite it with the correct value\n",
    "stats.loc[stats['config'].str.contains('Hydra'), 'normalized_energy'] = stats['total_energy'] / stats['ramulator.hydra_baseline_energy']\n",
    "\n",
    "# add the geometric normalized ipc average as a new workload to every config\n",
    "geometric_mean = stats.groupby(['config','nrh'])['normalized_energy'].apply(lambda x: x.prod()**(1.0/len(x))).reset_index()\n",
    "geometric_mean['workload'] = 'GeoMean'\n",
    "\n",
    "stats = pd.concat([stats, geometric_mean])\n",
    "# order in decreasing nRH (the nRH column) use pd.Categorical on nrh\n",
    "stats['nrh'] = pd.Categorical(stats['nrh'], categories=[1000, 500, 250, 125], ordered=True)\n",
    "\n",
    "\n",
    "#order = ['GeoMean', '531.deepsjeng', '502.gcc', '541.leela', '435.gromacs', '481.wrf', '458.sjeng', '445.gobmk', '444.namd', '508.namd', '401.bzip2', '456.hmmer', '403.gcc', '464.h264ref', '526.blender', '447.dealII', '544.nab', '523.xalancbmk', '500.perlbench', '538.imagick', '525.x264', '507.cactuBSSN', '511.povray', '462.libquantum', '473.astar', '510.parest', '482.sphinx3', '505.mcf', '557.xz', '471.omnetpp', '483.xalancbmk', '436.cactusADM', '520.omnetpp', '450.soplex', '470.lbm', '519.lbm', '434.zeusmp', '433.milc', '459.GemsFDTD', '549.fotonik3d', '429.mcf', '437.leslie3d']\n",
    "order = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp','GeoMean']\n",
    "\n",
    "# order = ['GeoMean', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', '500.perlbench', '523.xalancbmk', '510.parest', '557.xz', '482.sphinx3', '505.mcf', '436.cactusADM', '471.omnetpp', '473.astar', '483.xalancbmk', '462.libquantum', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf']\n",
    "\n",
    "stats['workload'] = pd.Categorical(stats['workload'], categories=order, ordered=True)\n",
    "#barplot of normalized IPC, also draw edges around bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 4))\n",
    "ax = sns.barplot(x='workload', y='normalized_energy', hue='nrh', data=stats[(stats['config'] == 'ABACuS')], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('Workload')\n",
    "ax.set_ylabel('Normalized DRAM Energy')\n",
    "# move ylabel down\n",
    "ax.yaxis.set_label_coords(-0.045,0.3)\n",
    "# draw a red line at y = 1.0, label it as baseline IPC\n",
    "ax.axhline(y=0.999, color='r', linestyle='--')\n",
    "# write above the red line 'baseline IPC' using the same pastel red color, also draw a box around it\n",
    "#ax.text(0.5, 0.99, 'baseline DRAM Energy', color='r', transform=ax.transAxes, bbox=dict(facecolor='white', edgecolor='r', boxstyle='round,pad=0.2'))\n",
    "ax.text(0.01, 0.2, 'Baseline DRAM energy', color='#e74c3c', transform=ax.transAxes, fontsize=15, bbox=dict(facecolor='white', edgecolor='r', boxstyle='round,pad=0.2', alpha = 0.95))\n",
    "\n",
    "# extend the y axis to 1.2\n",
    "ax.set_ylim(0.90, 1.2)\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[1].set_color('#e74c3c')\n",
    "# rotate x axis ticks\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "ax.tick_params(axis='y', which='major', labelsize=12)\n",
    "\n",
    "# # draw vertical lines to separate the rowhammer threshold values\n",
    "ax.axvline(x=26.5, color='grey', linestyle='-', alpha=0.5)\n",
    "# put text before the line saying \"LOW RBMPKI\"\n",
    "ax.text(0.33, 0.2, 'LOW RBMPKI', color='grey', transform=ax.transAxes, fontsize=12, bbox=dict(facecolor='white', edgecolor='grey', boxstyle='round,pad=0.2', alpha = 0.95))\n",
    "# put arrow to the left above text\n",
    "ax.annotate('', xy=(19.5, 0.93), xytext=(25.5, 0.93), arrowprops=dict(facecolor='grey', shrink=0.01, width=3, headwidth=10, alpha=0.99))\n",
    "ax.axvline(x=46.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.text(0.65, 0.2, 'MED. RBMPKI', color='grey', transform=ax.transAxes, fontsize=12, bbox=dict(facecolor='white', edgecolor='grey', boxstyle='round,pad=0.2', alpha = 0.99))\n",
    "ax.annotate('', xy=(39.5, 0.93), xytext=(45.5, 0.93), arrowprops=dict(facecolor='grey', shrink=0.01, width=3, headwidth=10, alpha=0.95))\n",
    "ax.axvline(x=61.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.text(0.875, 0.2, 'HIGH RBMPKI', color='grey', transform=ax.transAxes, fontsize=12, bbox=dict(facecolor='white', edgecolor='grey', boxstyle='round,pad=0.2', alpha = 0.99))\n",
    "ax.annotate('', xy=(54.5, 0.93), xytext=(60.5, 0.93), arrowprops=dict(facecolor='grey', shrink=0.01, width=3, headwidth=10, alpha=0.95))\n",
    "\n",
    "# make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(16)\n",
    "#ax.yaxis.label.set_fontweight('bold')\n",
    "ax.yaxis.label.set_size(16)\n",
    "#ax.xaxis.label.set_fontweight('bold')\n",
    "\n",
    "# put the legend on top of the plot\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4, fancybox=True, shadow=True, fontsize=12)\n",
    "# prepend \"nRH\" to legend names\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, ['$N_{RH}$ = ' + label for label in labels], loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=4, fancybox=True, shadow=True, fontsize=12)\n",
    "\n",
    "# highlight the geometric mean ax label\n",
    "ax.get_xticklabels()[62].set_fontweight('bold')\n",
    "\n",
    "# draw a small orange arrow to the middle of the plot\n",
    "ax.annotate('', xy=(54.5, 1.18), xytext=(56.5, 1.18), arrowprops=dict(facecolor='blue', shrink=0.01, width=3, headwidth=10, alpha=0.99))\n",
    "# put text above the arrow\n",
    "ax.text(0.835, 0.9, '2.01', color='blue', transform=ax.transAxes, fontsize=12, bbox=dict(facecolor='white', edgecolor='blue', boxstyle='round,pad=0.2', alpha = 0.99))\n",
    "\n",
    "# draw a small orange arrow to the middle of the plot\n",
    "ax.annotate('', xy=(54.5, 1.11), xytext=(56.8, 1.15), arrowprops=dict(facecolor='#e67e22', shrink=0.01, width=3, headwidth=10, alpha=0.99))\n",
    "# put text above the arrow\n",
    "ax.text(0.835, 0.6, '1.81', color='orange', transform=ax.transAxes, fontsize=12, bbox=dict(facecolor='white', edgecolor='orange', boxstyle='round,pad=0.2', alpha = 0.99))\n",
    "\n",
    "# draw a small orange arrow to the middle of the plot\n",
    "ax.annotate('', xy=(58.5, 1.115), xytext=(57.0, 1.15), arrowprops=dict(facecolor='green', shrink=0.01, width=3, headwidth=10, alpha=0.99))\n",
    "# put text above the arrow\n",
    "ax.text(0.935, 0.6, '1.71', color='green', transform=ax.transAxes, fontsize=12, bbox=dict(facecolor='white', edgecolor='green', boxstyle='round,pad=0.2', alpha = 0.99))\n",
    "\n",
    "# draw a small orange arrow to the middle of the plot\n",
    "ax.annotate('', xy=(59.5, 1.18), xytext=(57.3, 1.18), arrowprops=dict(facecolor='red', shrink=0.01, width=3, headwidth=10, alpha=0.99))\n",
    "# put text above the arrow\n",
    "ax.text(0.955, 0.9, '1.30', color='red', transform=ax.transAxes, fontsize=12, bbox=dict(facecolor='white', edgecolor='red', boxstyle='round,pad=0.2', alpha = 0.99))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_energy_single_core.pdf', bbox_inches='tight')\n",
    "# export data to csv\n",
    "stats.to_csv('abacus_energy_single_core.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# numbers to put in paper\n",
    "\n",
    "# ABACuS at 1000 nRH normalized_energy for the geomean workload\n",
    "print(stats[(stats['workload'] == 'GeoMean') & (stats['nrh'] == 1000) & (stats['config'] == 'ABACuS')]['normalized_energy'])\n",
    "# ABACuS at 1000 nRH maximum normalized_energy\n",
    "print(stats[(stats['nrh'] == 1000) & (stats['config'] == 'ABACuS')]['normalized_energy'].max())\n",
    "\n",
    "# the above for nrh = 125\n",
    "print(stats[(stats['workload'] == 'GeoMean') & (stats['nrh'] == 125) & (stats['config'] == 'ABACuS')]['normalized_energy'])\n",
    "print(stats[(stats['nrh'] == 125) & (stats['config'] == 'ABACuS')]['normalized_energy'].max())\n",
    "\n",
    "\n",
    "evaluation_paragraph = \"\"\"\n",
    "ABACuS average normalized energy at 1K nRH {avg_energy_1k}\n",
    "ABACuS max normalized energy at 1K nRH {max_energy_1k}\n",
    "\n",
    "ABACuS average normalized energy at 125 nRH {avg_energy_125}\n",
    "ABACuS max normalized energy at 125 nRH {max_energy_125}\n",
    "\"\"\".format(\n",
    "    avg_energy_1k=stats[(stats['nrh'] == 1000) & (stats['config'] == 'ABACuS')]['normalized_energy'].mean(),\n",
    "    max_energy_1k=stats[(stats['nrh'] == 1000) & (stats['config'] == 'ABACuS')]['normalized_energy'].max(),\n",
    "    avg_energy_125=stats[(stats['nrh'] == 125) & (stats['config'] == 'ABACuS')]['normalized_energy'].mean(),\n",
    "    max_energy_125=stats[(stats['nrh'] == 125) & (stats['config'] == 'ABACuS')]['normalized_energy'].max(),\n",
    ")\n",
    "\n",
    "print(evaluation_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance comparison (single-core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MECHANISM_NAME = 'ABACuS'\n",
    "\n",
    "### READ RESULTS INTO PANDAS DATAFRAME\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsdir = \"../results\"\n",
    "# list all directories in resultsdir\n",
    "configs = [d for d in os.listdir(resultsdir) if os.path.isdir(os.path.join(resultsdir, d))]\n",
    "configs = ['Baseline.yaml', 'Hydra-Baseline.yaml', 'REGA125.yaml', 'REGA250.yaml', 'REGA500.yaml', 'REGA1000.yaml', 'Graphene125.yaml', 'Graphene250.yaml', 'Graphene500.yaml', 'Graphene1000.yaml', 'Hydra125.yaml', 'Hydra250.yaml', 'Hydra500.yaml', 'Hydra1000.yaml', 'PARA125.yaml', 'PARA250.yaml', 'PARA500.yaml', 'PARA1000.yaml', \n",
    "           'ABACUS125.yaml', 'ABACUS250.yaml', 'ABACUS500.yaml', 'ABACUS1000.yaml', 'ABACUS125-Big.yaml', 'ABACUS250-Big.yaml', 'ABACUS500-Big.yaml', 'ABACUS1000-Big.yaml']\n",
    "# print found configs\n",
    "print('Found configs: {}'.format(configs))\n",
    "# list all directories under all configs\n",
    "workloads = []\n",
    "for c in configs:\n",
    "    workloads.append([d for d in os.listdir(os.path.join(resultsdir, c)) if os.path.isdir(os.path.join(resultsdir, c, d))])\n",
    "# find only the intersection of all workloads\n",
    "workloads = list(set.intersection(*map(set, workloads)))\n",
    "# print found workloads\n",
    "print('Found workloads: {}'.format(workloads))\n",
    "\n",
    "stats_per_config_workload = []\n",
    "workloads = [w for w in workloads if not '-' in w]\n",
    "\n",
    "# for every config + workload directory\n",
    "for c in configs:\n",
    "    for w in workloads:\n",
    "        # find all files in the directory\n",
    "        files = [f for f in os.listdir(os.path.join(resultsdir, c, w)) if os.path.isfile(os.path.join(resultsdir, c, w, f))]\n",
    "        # find the stats file\n",
    "        stat_files = [f for f in files if f.endswith('.stats')]\n",
    "        # if there is a stats file\n",
    "        if stat_files:\n",
    "            for stat_file in stat_files:\n",
    "                # if the stats_file has less than three lines skip it\n",
    "                if len(open(os.path.join(resultsdir, c, w, stat_file)).readlines()) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # print the name of the stats_file\n",
    "                print('Found stats file: {}'.format(os.path.join(os.path.join(resultsdir, c, w, stat_file))))\n",
    "\n",
    "                extension = ''\n",
    "                # if stats_file file name itself does not start with DDR4, parse it a bit\n",
    "                if not stat_file.startswith('DDR4'):\n",
    "                    # get the config name from the stats_file name\n",
    "                    extension = '_'.join(stat_file.split('_')[:-1])\n",
    "                    # prepend underscore to extension\n",
    "                    extension = '_' + extension\n",
    "\n",
    "                # read the stats file, name columns: 'name', 'value', 'description'\n",
    "                df = pd.read_csv(os.path.join(resultsdir, c, w, stat_file), header=None).T\n",
    "                df.columns = df.iloc[0]\n",
    "                df.drop(0,inplace=True)\n",
    "                # add a new column called 'config' with the config name\n",
    "                df['config'] = c + extension\n",
    "                # add a new column called 'workload' with the workload name\n",
    "                df['workload'] = w\n",
    "                # print the stats file\n",
    "                # print('Config: {}, Workload: {}, Stats: {}'.format(c, w, df))\n",
    "                # append the stats to the list\n",
    "                df.reset_index(inplace=True, drop=True)\n",
    "                stats_per_config_workload.append(df)\n",
    "        else:\n",
    "            print('Config: {}, Workload: {}, Stats: No stats file found'.format(c, w))\n",
    "\n",
    "# concatenate all stats into one dataframe\n",
    "stats = pd.concat(stats_per_config_workload)\n",
    "\n",
    "# find elements where workload does not contain '-'\n",
    "# these are multi core workloads\n",
    "stats = stats[~stats['workload'].str.contains('-')]\n",
    "\n",
    "# remove these two workloads: stream_10.trace and random_10.trace\n",
    "stats = stats[~stats['workload'].isin(['gups'])]\n",
    "# also from workloads\n",
    "workloads = [w for w in workloads if not w in ['gups']]\n",
    "\n",
    "# make sure config is string\n",
    "stats['config'] = stats['config'].astype(str)\n",
    "\n",
    "# remove \"-16DR\" from config names\n",
    "stats['config'] = stats['config'].str.replace('-16DR', '')\n",
    "\n",
    "# replace 1K with 1000 in config names\n",
    "stats['config'] = stats['config'].str.replace('1K', '1000')\n",
    "\n",
    "# replace 'Baseline' with 'Baseline0'\n",
    "stats['config'] = stats['config'].str.replace('Baseline', 'Baseline0')\n",
    "\n",
    "# add a new column that stores in integer the number in the config name\n",
    "stats['nrh'] = stats['config'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# remove numbers from config names\n",
    "stats['config'] = stats['config'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# remove yaml from config names\n",
    "stats['config'] = stats['config'].str.replace('.yaml', '')\n",
    "\n",
    "# replace SPR with APAR\n",
    "stats['config'] = stats['config'].str.replace('ABACUS', MY_MECHANISM_NAME)\n",
    "\n",
    "stats.loc[stats['workload'] == 'random_10.trace', 'workload'] = 'gups'\n",
    "\n",
    "# increasing order of rbmpki\n",
    "# order = ['511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', '500.perlbench', '523.xalancbmk', '510.parest', '557.xz', '482.sphinx3', '505.mcf', '436.cactusADM', '471.omnetpp', '473.astar', '483.xalancbmk', '462.libquantum', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf']\n",
    "order = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "# remove all workloads not in order\n",
    "stats = stats[stats['workload'].isin(order)]\n",
    "# also from the workload list\n",
    "workloads = [w for w in workloads if w in order]\n",
    "\n",
    "# order workloads according to the order\n",
    "stats['workload'] = pd.Categorical(stats['workload'], categories=order, ordered=True)\n",
    "\n",
    "stats_copy = stats.copy()\n",
    "\n",
    "# import MultipleLocator\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=5)\n",
    "\n",
    "stats = stats_copy.copy()\n",
    "# instructions per cycle (IPC) is record_cycles_insts_0 / record_cycs_core_0\n",
    "stats['ramulator.ipc'] = stats['ramulator.record_insts_core_0'] / stats['ramulator.record_cycs_core_0']\n",
    "\n",
    "\n",
    "stats['ramulator.rbmpki'] = (stats['ramulator.row_conflicts_channel_0_core'] + stats['ramulator.row_misses_channel_0_core']) /\\\n",
    "                            stats['ramulator.record_insts_core_0'] * 1000\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline = stats[stats['config'] == 'Baseline']\n",
    "baseline = baseline[['workload', 'ramulator.ipc', 'ramulator.read_latency_avg_0', 'ramulator.rbmpki', 'ramulator.window_full_stall_cycles_core_0']]\n",
    "# baseline\n",
    "baseline.columns = ['workload', 'ramulator.baseline_ipc', 'ramulator.baseline_read_latency_avg_0', 'ramulator.baseline_rbmpki', 'ramulator.baseline_stall_cycles']\n",
    "stats = pd.merge(stats, baseline, on='workload')\n",
    "#hydra baseline\n",
    "hydra_baseline = stats[stats['config'] == 'Hydra-Baseline']\n",
    "hydra_baseline = hydra_baseline[['workload', 'ramulator.ipc']]\n",
    "# hydra_baseline\n",
    "hydra_baseline.columns = ['workload', 'ramulator.hydra_baseline_ipc']\n",
    "stats = pd.merge(stats, hydra_baseline, on='workload')\n",
    "\n",
    "stats['ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc']\n",
    "stats['ramulator.normalized_read_latency'] = stats['ramulator.read_latency_avg_0'] / stats['ramulator.baseline_read_latency_avg_0']\n",
    "stats['ramulator.normalized_stall_cycles'] = stats['ramulator.window_full_stall_cycles_core_0'] / stats['ramulator.baseline_stall_cycles']\n",
    "stats['ramulator.normalized_rbmpki'] = stats['ramulator.rbmpki'] / stats['ramulator.baseline_rbmpki']\n",
    "\n",
    "# # instructions per cycle (IPC) is record_cycles_insts_0 / record_cycs_core_0\n",
    "# stats['ramulator.ipc'] = stats['ramulator.record_insts_core_0'] / stats['ramulator.record_cycs_core_0']\n",
    "\n",
    "# # copy the IPC of the baseline config as to all configs\n",
    "# baseline = stats[stats['config'] == 'Baseline']\n",
    "# baseline = baseline[['workload', 'ramulator.ipc']]\n",
    "# # baseline\n",
    "# baseline.columns = ['workload', 'ramulator.baseline_ipc']\n",
    "# stats = pd.merge(stats, baseline, on='workload')\n",
    "\n",
    "# #hydra baseline\n",
    "# hydra_baseline = stats[stats['config'] == 'Hydra-Baseline']\n",
    "# hydra_baseline = hydra_baseline[['workload', 'ramulator.ipc']]\n",
    "# # hydra_baseline\n",
    "# hydra_baseline.columns = ['workload', 'ramulator.hydra_baseline_ipc']\n",
    "# stats = pd.merge(stats, hydra_baseline, on='workload')\n",
    "\n",
    "# stats['ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc']\n",
    "\n",
    "# normalized ipc for hydra is not correct, so we overwrite it with the correct value\n",
    "stats.loc[stats['config'].str.contains('Hydra'), 'ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.hydra_baseline_ipc']\n",
    "\n",
    "\n",
    "# new dataframe that does not have the baseline configs\n",
    "stats_no_baseline = stats[~stats['config'].str.contains('Baseline')]\n",
    "\n",
    "# new dataframe that does not have the baseline configs\n",
    "print(stats_no_baseline['config'].unique())\n",
    "\n",
    "# order nRH from high to low\n",
    "stats_no_baseline['nrh'] = pd.Categorical(stats_no_baseline['nrh'], categories=[1000, 500, 250, 125], ordered=True)\n",
    "\n",
    "# order config in this order: abacus, Graphene, Hydra, REGA, PARA\n",
    "stats_no_baseline['config'] = pd.Categorical(stats_no_baseline['config'], categories=['ABACuS', 'Graphene', 'Hydra', 'REGA', 'PARA'], ordered=True)\n",
    "\n",
    "#boxplot of normalized IPC\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "# show mean values as well\n",
    "ax = sns.boxplot(x=\"nrh\", y=\"ramulator.normalized_ipc\", hue=\"config\", data=stats_no_baseline, showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\"})\n",
    "ax.set_xlabel('RowHammer Threshold ($N_{RH}$)')\n",
    "ax.set_ylabel('Normalized IPC Distribution')\n",
    "# draw a red line at y = 1.0, label it as baseline IPC\n",
    "ax.axhline(y=1.0, color='r', linestyle='--')\n",
    "# write above the red line 'baseline IPC'\n",
    "ax.text(0.02, 0.92, 'Baseline IPC', color='#e74c3c', transform=ax.transAxes, fontsize=15)\n",
    "# extend the y axis to 1.2\n",
    "ax.set_ylim(0.2, 1.1)\n",
    "# y axis tick every 0.1 increment\n",
    "ax.yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[9].set_color('#e74c3c')\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "# draw vertical lines to separate the rowhammer threshold values\n",
    "ax.axvline(x=0.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=1.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=2.5, color='grey', linestyle='-', alpha=0.5)\n",
    "# make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(16)\n",
    "ax.yaxis.label.set_size(16)\n",
    "\n",
    "# put the legend on top of the plot\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=5, fancybox=True, shadow=True, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_performance_comparison_single_core.pdf', bbox_inches='tight')\n",
    "\n",
    "# list mean normalized_ipc at 1000 nRH for all configs\n",
    "# print(stats_no_baseline.groupby(['config','nrh'])['ramulator.normalized_ipc'].mean())\n",
    "\n",
    "# Average normalized IPC for REGA at nRH = 125\n",
    "print(1-stats_no_baseline[(stats_no_baseline['config'] == 'REGA') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_ipc'].mean())\n",
    "# Same for PARA at 1000 and 125\n",
    "print(1-stats_no_baseline[(stats_no_baseline['config'] == 'PARA') & (stats_no_baseline['nrh'] == 1000)]['ramulator.normalized_ipc'].mean())\n",
    "print(1-stats_no_baseline[(stats_no_baseline['config'] == 'PARA') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_ipc'].mean())\n",
    "# Average normalized IPC for ABACuS divided by Hydra at each nRH\n",
    "print(1/stats_no_baseline[(stats_no_baseline['config'] == 'Hydra') & (stats_no_baseline['nrh'] == 1000)]['ramulator.normalized_ipc'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 1000)]['ramulator.normalized_ipc'].mean())\n",
    "print(1/stats_no_baseline[(stats_no_baseline['config'] == 'Hydra') & (stats_no_baseline['nrh'] == 500)]['ramulator.normalized_ipc'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 500)]['ramulator.normalized_ipc'].mean())\n",
    "print(1/stats_no_baseline[(stats_no_baseline['config'] == 'Hydra') & (stats_no_baseline['nrh'] == 250)]['ramulator.normalized_ipc'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 250)]['ramulator.normalized_ipc'].mean())\n",
    "print(1/stats_no_baseline[(stats_no_baseline['config'] == 'Hydra') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_ipc'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_ipc'].mean())\n",
    "\n",
    "# average rbmpki of Hydra at nRH 125 divided by ABACuS at nRH 125\n",
    "print(1/(stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_rbmpki'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'Hydra') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_rbmpki'].mean()))\n",
    "# same for memory latency\n",
    "print(1/(stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_read_latency'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'Hydra') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_read_latency'].mean()))\n",
    "\n",
    "# mean number of preventive refreshes for ABACuS at 125 nrh divided by graphene\n",
    "print(stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 125)]['ramulator.preventive_refreshes_channel_0_core'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'Graphene') & (stats_no_baseline['nrh'] == 125)]['ramulator.preventive_refreshes_channel_0_core'].mean())\n",
    "# mean number of stall cycles for the same\n",
    "print(1/(stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_stall_cycles'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'Graphene') & (stats_no_baseline['nrh'] == 125)]['ramulator.normalized_stall_cycles'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance comparison (8-cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MECHANISM_NAME = 'ABACuS'\n",
    "\n",
    "### READ RESULTS INTO PANDAS DATAFRAME\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsdir = \"../results\"\n",
    "# list all directories in resultsdir\n",
    "configs = [d for d in os.listdir(resultsdir) if os.path.isdir(os.path.join(resultsdir, d))]\n",
    "configs = ['Baseline.yaml', 'MC-Baseline.yaml', 'Hydra-Baseline.yaml', 'REGA125.yaml', 'REGA250.yaml', 'REGA500.yaml', 'REGA1000.yaml', 'Graphene125.yaml', 'Graphene250.yaml', 'Graphene500.yaml', 'Graphene1000.yaml', 'Hydra125.yaml', 'Hydra250.yaml', 'Hydra500.yaml', 'Hydra1000.yaml', 'PARA125.yaml', 'PARA250.yaml', 'PARA500.yaml', 'PARA1000.yaml', \n",
    "           'ABACUS125.yaml', 'ABACUS250.yaml', 'ABACUS500.yaml', 'ABACUS1000.yaml', 'ABACUS125-Big.yaml', 'ABACUS250-Big.yaml', 'ABACUS500-Big.yaml', 'ABACUS1000-Big.yaml']\n",
    "# print found configs\n",
    "print('Found configs: {}'.format(configs))\n",
    "# list all directories under all configs\n",
    "workloads = []\n",
    "for c in configs:\n",
    "    workloads.append([d for d in os.listdir(os.path.join(resultsdir, c)) if os.path.isdir(os.path.join(resultsdir, c, d))])\n",
    "# find only the intersection of all workloads\n",
    "# workloads = list(set.intersection(*map(set, workloads)))\n",
    "# keep only unique workloads\n",
    "workloads = list(set([item for sublist in workloads for item in sublist]))\n",
    "\n",
    "# print found workloads\n",
    "print('Found workloads: {}'.format(workloads))\n",
    "\n",
    "stats_per_config_workload = []\n",
    "\n",
    "# for every config + workload directory\n",
    "for c in configs:\n",
    "    for w in workloads:\n",
    "        # find all files in the directory\n",
    "        if os.path.isdir(os.path.join(resultsdir, c, w)):\n",
    "            files = [f for f in os.listdir(os.path.join(resultsdir, c, w)) if os.path.isfile(os.path.join(resultsdir, c, w, f))]\n",
    "            # find the stats file\n",
    "            stat_files = [f for f in files if f.endswith('.stats')]\n",
    "            # if there is a stats file\n",
    "            if stat_files:\n",
    "                for stat_file in stat_files:\n",
    "                    # if the stats_file has less than three lines skip it\n",
    "                    if len(open(os.path.join(resultsdir, c, w, stat_file)).readlines()) < 3:\n",
    "                        continue\n",
    "                    \n",
    "                    # print the name of the stats_file\n",
    "                    print('Found stats file: {}'.format(os.path.join(os.path.join(resultsdir, c, w, stat_file))))\n",
    "\n",
    "                    extension = ''\n",
    "                    # if stats_file file name itself does not start with DDR4, parse it a bit\n",
    "                    if not stat_file.startswith('DDR4'):\n",
    "                        # get the config name from the stats_file name\n",
    "                        extension = '_'.join(stat_file.split('_')[:-1])\n",
    "                        # prepend underscore to extension\n",
    "                        extension = '_' + extension\n",
    "\n",
    "                    # read the stats file, name columns: 'name', 'value', 'description'\n",
    "                    df = pd.read_csv(os.path.join(resultsdir, c, w, stat_file), header=None).T\n",
    "                    df.columns = df.iloc[0]\n",
    "                    df.drop(0,inplace=True)\n",
    "                    # add a new column called 'config' with the config name\n",
    "                    df['config'] = c + extension\n",
    "                    # add a new column called 'workload' with the workload name\n",
    "                    df['workload'] = w\n",
    "                    # print the stats file\n",
    "                    # print('Config: {}, Workload: {}, Stats: {}'.format(c, w, df))\n",
    "                    # append the stats to the list\n",
    "                    df.reset_index(inplace=True, drop=True)\n",
    "                    stats_per_config_workload.append(df)\n",
    "            else:\n",
    "                print('Config: {}, Workload: {}, Stats: No stats file found'.format(c, w))\n",
    "\n",
    "# concatenate all stats into one dataframe\n",
    "stats = pd.concat(stats_per_config_workload)\n",
    "\n",
    "# remove workloads that have \"gups\" in them\n",
    "stats = stats[~stats['workload'].str.contains('gups')]\n",
    "\n",
    "# grep_map0 produced 0 cycles for any core except 7, weird bug, ignore for now\n",
    "# stats = stats[~stats['workload'].str.contains('grep_map0')]\n",
    "\n",
    "# also remove them from the list\n",
    "workloads = [w for w in workloads if 'gups' not in w]\n",
    "\n",
    "# grep_map0 produced 0 cycles for any core except 7, weird bug, ignore for now\n",
    "# workloads = [w for w in workloads if 'grep_map0' not in w]\n",
    "\n",
    "\n",
    "# replace all instances of \"random_10.trace\" in workload names with \"gups\"\n",
    "stats['workload'] = stats['workload'].str.replace('random_10.trace', 'gups')\n",
    "\n",
    "# all workloads\n",
    "HIGH_RBMPKI = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "# HIGH_RBMPKI = ['519.lbm', '459.GemsFDTD', '450.soplex', 'h264_decode', '520.omnetpp', '433.milc', '434.zeusmp', 'bfs_dblp', '429.mcf', '549.fotonik3d', '470.lbm', 'bfs_ny', 'bfs_cm2003', '437.leslie3d', 'gups']\n",
    "\n",
    "# HIGH_RBMPKI = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'random_10.trace', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "#HIGH_RBMPKI = ['531.deepsjeng', '502.gcc', '541.leela', '435.gromacs', '481.wrf', '458.sjeng', '445.gobmk', '444.namd', '508.namd', '401.bzip2', '456.hmmer', '403.gcc', '464.h264ref', '526.blender', '447.dealII', '544.nab', '523.xalancbmk', '500.perlbench', '538.imagick', '525.x264', '507.cactuBSSN', '511.povray', '462.libquantum', '473.astar', '510.parest', '482.sphinx3', '505.mcf', '557.xz', '471.omnetpp', '483.xalancbmk', '436.cactusADM', '520.omnetpp', '450.soplex', '470.lbm', '519.lbm', '434.zeusmp', '433.milc', '459.GemsFDTD', '549.fotonik3d', '429.mcf', '437.leslie3d']\n",
    "stats = stats[stats['workload'].str.contains('|'.join(HIGH_RBMPKI))]\n",
    "\n",
    "# remove h264_decode fro workloads\n",
    "# stats = stats[~stats['workload'].str.contains('h264_decode')]\n",
    "# remove h264_decode from HIGH_RBMPKI\n",
    "# HIGH_RBMPKI.remove('h264_decode')\n",
    "\n",
    "# keep mc_only_stats only for workloads that contain the strings in HIGH_RBMPKI\n",
    "\n",
    "# remove \"-16DR\" from config names\n",
    "stats['config'] = stats['config'].str.replace('-16DR', '')\n",
    "\n",
    "# replace 1K with 1000 in config names\n",
    "stats['config'] = stats['config'].str.replace('1K', '1000')\n",
    "\n",
    "# replace 'Baseline' with 'Baseline0'\n",
    "stats['config'] = stats['config'].str.replace('Baseline', 'Baseline0')\n",
    "\n",
    "# add a new column that stores in integer the number in the config name\n",
    "stats['nrh'] = stats['config'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# remove numbers from config names\n",
    "stats['config'] = stats['config'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# remove yaml from config names\n",
    "stats['config'] = stats['config'].str.replace('.yaml', '')\n",
    "\n",
    "# replace SPR with APAR\n",
    "stats['config'] = stats['config'].str.replace('ABACUS', MY_MECHANISM_NAME)\n",
    "\n",
    "stats_copy = stats.copy()\n",
    "\n",
    "# remove from mc_only_stats the workloads that contain less than 7 dashes\n",
    "mc_only_stats = stats[stats['workload'].str.count('-') >= 7]\n",
    "# remove from mc_only_stats workloads that contain stream_10.trace or random_10.trace\n",
    "mc_only_stats = mc_only_stats[~mc_only_stats['workload'].str.contains('stream_10.trace')]\n",
    "mc_only_stats = mc_only_stats[~mc_only_stats['workload'].str.contains('random_10.trace')]\n",
    "sc_only_stats = stats[~stats['workload'].str.contains('-')].copy()\n",
    "\n",
    "# expand the workload column into four columns by splitting using '-'\n",
    "mc_only_stats[['wl0', 'wl1', 'wl2', 'wl3', 'wl4', 'wl5', 'wl6', 'wl7']] = mc_only_stats['workload'].str.split('-', expand=True)\n",
    "sc_only_stats['ramulator.ipc'] = sc_only_stats['ramulator.record_insts_core_0'] / sc_only_stats['ramulator.record_cycs_core_0']\n",
    "\n",
    "# for each ramulator.record_insts_core_i column, if ramulator.record_cycs_core_i is 0, set it to 1 (This is to prevent division by zero)\n",
    "for i in range(0, 8):\n",
    "    mc_only_stats.loc[mc_only_stats['ramulator.record_cycs_core_{}'.format(i)] == 0, 'ramulator.record_cycs_core_{}'.format(i)] = 1\n",
    "\n",
    "# for each ramulator.record_insts_core_i column, divide it by ramulator.record_cycs_core_i column and add it as ipci column\n",
    "for i in range(0, 8):\n",
    "    mc_only_stats['ipc{}'.format(i)] = mc_only_stats['ramulator.record_insts_core_{}'.format(i)] / mc_only_stats['ramulator.record_cycs_core_{}'.format(i)]\n",
    "\n",
    "# write to csv mc_only_stats but only the config, workload and ipci columns\n",
    "mc_only_stats[['config', 'wl0', 'wl1', 'wl2', 'wl3', 'wl4', 'wl5', 'wl6', 'wl7', 'ipc0', 'ipc1', 'ipc2', 'ipc3', 'ipc4', 'ipc5', 'ipc6', 'ipc7', 'nrh']].to_csv('mc_only_stats.csv', index=False)\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=5)\n",
    "\n",
    "stats_parsed = pd.read_csv('mc_only_stats.csv')\n",
    "\n",
    "stats_parsed['workload'] = stats_parsed['wl0']\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline = sc_only_stats[sc_only_stats['config'] == 'MC-Baseline']\n",
    "baseline = baseline[['workload', 'ramulator.ipc']]\n",
    "\n",
    "# baseline\n",
    "baseline.columns = ['workload', 'ramulator.baseline_ipc']\n",
    "stats_parsed = pd.merge(stats_parsed, baseline, on='workload')\n",
    "\n",
    "# for all ipc columns, divide by the baseline ipc\n",
    "for i in range(0, 8):\n",
    "    stats_parsed['normalized_ipc{}'.format(i)] = stats_parsed['ipc{}'.format(i)] / stats_parsed['ramulator.baseline_ipc']\n",
    "\n",
    "stats_parsed['weighted_speedup'] = stats_parsed['normalized_ipc0'] + stats_parsed['normalized_ipc1'] + stats_parsed['normalized_ipc2'] + stats_parsed['normalized_ipc3'] + stats_parsed['normalized_ipc4'] + stats_parsed['normalized_ipc5'] + stats_parsed['normalized_ipc6'] + stats_parsed['normalized_ipc7']\n",
    "\n",
    "baselinepp = stats_parsed[stats_parsed['config'] == 'Baseline']\n",
    "baselinepp = baselinepp[['workload', 'weighted_speedup']]\n",
    "# baseline\n",
    "baselinepp.columns = ['workload', 'baseline_weighted_speedup']\n",
    "stats_parsed = pd.merge(stats_parsed, baselinepp, on='workload')\n",
    "\n",
    "stats_parsed['normalized_weighted_speedup'] = stats_parsed['weighted_speedup'] / stats_parsed['baseline_weighted_speedup']\n",
    "\n",
    "# for normalized_weighted_speedups above 1.0, make them 1.0\n",
    "# stats_parsed.loc[stats_parsed['normalized_weighted_speedup'] > 1.0, 'normalized_weighted_speedup'] = 1.0\n",
    "\n",
    "stats_parsed = stats_parsed[~stats_parsed['config'].str.contains('Baseline')]\n",
    "\n",
    "# print(stats_parsed['workload'].unique())\n",
    "\n",
    "# order nRH from high to low\n",
    "stats_parsed['nrh'] = pd.Categorical(stats_parsed['nrh'], categories=[1000, 500, 250, 125], ordered=True)\n",
    "\n",
    "# order config in this order: abacus, Graphene, Hydra, REGA, PARA\n",
    "stats_parsed['config'] = pd.Categorical(stats_parsed['config'], categories=['ABACuS', 'Graphene', 'Hydra', 'REGA', 'PARA'], ordered=True)\n",
    "\n",
    "#boxplot of normalized IPC\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# show mean values do not plot outliers\n",
    "ax = sns.boxplot(x=\"nrh\", y=\"normalized_weighted_speedup\", hue=\"config\", data=stats_parsed, showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\"})#, showfliers=False)\n",
    "\n",
    "ax.set_xlabel('RowHammer Threshold ($N_{RH}$)')\n",
    "ax.set_ylabel('Normalized Weighted Speedup')\n",
    "# draw a red line at y = 1.0, label it as baseline IPC\n",
    "ax.axhline(y=1.0, color='r', linestyle='--')\n",
    "# write above the red line 'baseline IPC'\n",
    "ax.text(0.02, 0.94, 'Baseline weighted speedup', color='#e74c3c', transform=ax.transAxes, fontsize=15)\n",
    "# extend the y axis to 1.2\n",
    "ax.set_ylim(0, 1.1)\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[5].set_color('#e74c3c')\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "# draw vertical lines to separate the rowhammer threshold values\n",
    "ax.axvline(x=0.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=1.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=2.5, color='grey', linestyle='-', alpha=0.5)\n",
    "# make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(16)\n",
    "ax.yaxis.label.set_size(16)\n",
    "\n",
    "# put the legend on top of the plot\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=5, fancybox=True, shadow=True, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_performance_comparison_multi_core.pdf', bbox_inches='tight')\n",
    "# export csv to file\n",
    "stats_parsed.to_csv('abacus_performance_comparison_multi_core.csv', index=False)\n",
    "\n",
    "\n",
    "# average normalized ws across all workloads for ABACuS at 1K nRH (print in one line)\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'ABACuS') & (stats_parsed['nrh'] == 1000)]['normalized_weighted_speedup'].mean()))\n",
    "# same for other 3 thresholds\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'ABACuS') & (stats_parsed['nrh'] == 500)]['normalized_weighted_speedup'].mean()))\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'ABACuS') & (stats_parsed['nrh'] == 250)]['normalized_weighted_speedup'].mean()))\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'ABACuS') & (stats_parsed['nrh'] == 125)]['normalized_weighted_speedup'].mean()))\n",
    "\n",
    "#Hydra's average normalized ws across all workloads at 1K nRH (print in one line) divided by ABACuS's\n",
    "print(-(stats_parsed[(stats_parsed['config'] == 'Hydra') & (stats_parsed['nrh'] == 1000)]['normalized_weighted_speedup'].mean())+(stats_parsed[(stats_parsed['config'] == 'ABACuS') & (stats_parsed['nrh'] == 1000)]['normalized_weighted_speedup'].mean()))\n",
    "\n",
    "print(1/(stats_parsed[(stats_parsed['config'] == 'ABACuS') & (stats_parsed['nrh'] == 125)]['normalized_weighted_speedup'].mean()))\n",
    "# Just Hydra's overhead at nRH 125\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'Hydra') & (stats_parsed['nrh'] == 125)]['normalized_weighted_speedup'].mean()))\n",
    "# REGA's\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'REGA') & (stats_parsed['nrh'] == 125)]['normalized_weighted_speedup'].mean()))\n",
    "# PARA's\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'PARA') & (stats_parsed['nrh'] == 125)]['normalized_weighted_speedup'].mean()))\n",
    "\n",
    "# Graphene's at nRH 1000\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'Graphene') & (stats_parsed['nrh'] == 1000)]['normalized_weighted_speedup'].mean()))\n",
    "\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'ABACuS') & (stats_parsed['nrh'] == 1000)]['normalized_weighted_speedup'].min()))\n",
    "\n",
    "# Hydra max overhead at 125\n",
    "print(1-(stats_parsed[(stats_parsed['config'] == 'Hydra') & (stats_parsed['nrh'] == 125)]['normalized_weighted_speedup'].min()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy comparison (single-core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MECHANISM_NAME = 'ABACuS'\n",
    "\n",
    "### READ RESULTS INTO PANDAS DATAFRAME\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsdir = \"../results\"\n",
    "# list all directories in resultsdir\n",
    "configs = [d for d in os.listdir(resultsdir) if os.path.isdir(os.path.join(resultsdir, d))]\n",
    "configs = ['Baseline.yaml', 'Hydra-Baseline.yaml', 'REGA125.yaml', 'REGA250.yaml', 'REGA500.yaml', 'REGA1000.yaml', 'Graphene125.yaml', 'Graphene250.yaml', 'Graphene500.yaml', 'Graphene1000.yaml', 'Hydra125.yaml', 'Hydra250.yaml', 'Hydra500.yaml', 'Hydra1000.yaml', 'PARA125.yaml', 'PARA250.yaml', 'PARA500.yaml', 'PARA1000.yaml', 'ABACUS125.yaml', 'ABACUS250.yaml', 'ABACUS500.yaml', 'ABACUS1000.yaml']\n",
    "\n",
    "# print found configs\n",
    "print('Found configs: {}'.format(configs))\n",
    "# list all directories under all configs\n",
    "workloads = []\n",
    "for c in configs:\n",
    "    workloads.append([d for d in os.listdir(os.path.join(resultsdir, c)) if os.path.isdir(os.path.join(resultsdir, c, d))])\n",
    "# find only the intersection of all workloads\n",
    "workloads = list(set.intersection(*map(set, workloads)))\n",
    "# print found workloads\n",
    "print('Found workloads: {}'.format(workloads))\n",
    "\n",
    "stats_per_config_workload = []\n",
    "workloads = [w for w in workloads if not '-' in w]\n",
    "\n",
    "# for every config + workload directory\n",
    "for c in configs:\n",
    "    for w in workloads:\n",
    "        # find all files in the directory\n",
    "        files = [f for f in os.listdir(os.path.join(resultsdir, c, w)) if os.path.isfile(os.path.join(resultsdir, c, w, f))]\n",
    "        # find the stats file\n",
    "        stat_files = [f for f in files if f.endswith('output.txt')]\n",
    "        # if there is a stats file\n",
    "        if stat_files:\n",
    "            for stat_file in stat_files:\n",
    "                # if the stats_file has less than three lines skip it\n",
    "                if len(open(os.path.join(resultsdir, c, w, stat_file)).readlines()) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # print the name of the stats_file\n",
    "                print('Found stats file: {}'.format(os.path.join(os.path.join(resultsdir, c, w, stat_file))))\n",
    "\n",
    "                lines = open(os.path.join(resultsdir, c, w, stat_file)).readlines()\n",
    "                total_energy = 0\n",
    "                for l in lines:\n",
    "                    # if line contains nJ, add l.split()[-2] to total_energy\n",
    "                    if 'Total Idle energy:' in l:\n",
    "                        continue\n",
    "                    if 'nJ' in l:\n",
    "                        total_energy += float(l.split()[-2])\n",
    "                    if l.startswith('REF CMD energy'):\n",
    "                        break\n",
    "\n",
    "                \n",
    "                # create a df with the config, workload and total_energy\n",
    "                df = pd.DataFrame({'config': [c], 'workload': [w], 'total_energy': [total_energy]})\n",
    "                df.reset_index(inplace=True, drop=True)\n",
    "                stats_per_config_workload.append(df)\n",
    "        else:\n",
    "            print('Config: {}, Workload: {}, Stats: No stats file found'.format(c, w))\n",
    "\n",
    "# concatenate all stats into one dataframe\n",
    "stats = pd.concat(stats_per_config_workload)\n",
    "\n",
    "# find elements where workload does not contain '-'\n",
    "# these are multi core workloads\n",
    "stats = stats[~stats['workload'].str.contains('-')]\n",
    "\n",
    "# remove these two workloads: stream_10.trace and random_10.trace\n",
    "stats = stats[~stats['workload'].isin(['gups'])]\n",
    "# also from workloads\n",
    "workloads = [w for w in workloads if not w in ['gups']]\n",
    "\n",
    "# remove \"-16DR\" from config names\n",
    "stats['config'] = stats['config'].str.replace('-16DR', '')\n",
    "\n",
    "# replace 1K with 1000 in config names\n",
    "stats['config'] = stats['config'].str.replace('1K', '1000')\n",
    "\n",
    "# replace 'Baseline' with 'Baseline0'\n",
    "stats['config'] = stats['config'].str.replace('Baseline', 'Baseline0')\n",
    "\n",
    "# add a new column that stores in integer the number in the config name\n",
    "stats['nrh'] = stats['config'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# remove numbers from config names\n",
    "stats['config'] = stats['config'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# remove yaml from config names\n",
    "stats['config'] = stats['config'].str.replace('.yaml', '')\n",
    "\n",
    "# replace SPR with APAR\n",
    "stats['config'] = stats['config'].str.replace('ABACUS', MY_MECHANISM_NAME)\n",
    "\n",
    "stats.loc[stats['workload'] == 'random_10.trace', 'workload'] = 'gups'\n",
    "\n",
    "# increasing order of rbmpki\n",
    "#order = ['511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', '500.perlbench', '523.xalancbmk', '510.parest', '557.xz', '482.sphinx3', '505.mcf', '436.cactusADM', '471.omnetpp', '473.astar', '483.xalancbmk', '462.libquantum', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf']\n",
    "order = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "\n",
    "# remove all workloads not in order\n",
    "stats = stats[stats['workload'].isin(order)]\n",
    "# also from the workload list\n",
    "workloads = [w for w in workloads if w in order]\n",
    "\n",
    "\n",
    "# order workloads according to the order\n",
    "stats['workload'] = pd.Categorical(stats['workload'], categories=order, ordered=True)\n",
    "\n",
    "stats_copy = stats.copy()\n",
    "\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=5)\n",
    "\n",
    "stats = stats_copy.copy()\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline = stats[stats['config'] == 'Baseline']\n",
    "baseline = baseline[['workload', 'total_energy']]\n",
    "# baseline\n",
    "baseline.columns = ['workload', 'baseline_energy']\n",
    "stats = pd.merge(stats, baseline, on='workload')\n",
    "\n",
    "#hydra baseline\n",
    "hydra_baseline = stats[stats['config'] == 'Hydra-Baseline']\n",
    "hydra_baseline = hydra_baseline[['workload', 'total_energy']]\n",
    "# hydra_baseline\n",
    "hydra_baseline.columns = ['workload', 'ramulator.hydra_baseline_energy']\n",
    "stats = pd.merge(stats, hydra_baseline, on='workload')\n",
    "\n",
    "stats['normalized_energy'] = stats['total_energy'] / stats['baseline_energy']\n",
    "\n",
    "# normalized ipc for hydra is not correct, so we overwrite it with the correct value\n",
    "stats.loc[stats['config'].str.contains('Hydra'), 'normalized_energy'] = stats['total_energy'] / stats['ramulator.hydra_baseline_energy']\n",
    "\n",
    "# new dataframe that does not have the baseline configs\n",
    "stats_no_baseline = stats[~stats['config'].str.contains('Baseline')]\n",
    "\n",
    "# new dataframe that does not have the baseline configs\n",
    "print(stats_no_baseline['config'].unique())\n",
    "\n",
    "# order nRH from high to low\n",
    "stats_no_baseline['nrh'] = pd.Categorical(stats_no_baseline['nrh'], categories=[1000, 500, 250, 125], ordered=True)\n",
    "\n",
    "# order config in this order: abacus, Graphene, Hydra, REGA, PARA\n",
    "stats_no_baseline['config'] = pd.Categorical(stats_no_baseline['config'], categories=['ABACuS', 'Graphene', 'Hydra', 'REGA', 'PARA'], ordered=True)\n",
    "\n",
    "#boxplot of normalized IPC\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "# show mean values as well\n",
    "ax = sns.boxplot(x=\"nrh\", y=\"normalized_energy\", hue=\"config\", data=stats_no_baseline, showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\"}, showfliers = True)\n",
    "ax.set_xlabel('RowHammer Threshold ($N_{RH}$)')\n",
    "ax.set_ylabel('Normalized Energy Distribution')\n",
    "# draw a red line at y = 1.0, label it as baseline IPC\n",
    "ax.axhline(y=1.0, color='r', linestyle='--')\n",
    "# write above the red line 'baseline IPC'\n",
    "ax.text(0.02, 0.02, 'Baseline DRAM energy', color='#e74c3c', transform=ax.transAxes, fontsize=15)\n",
    "# extend the y axis to 1.2\n",
    "ax.set_ylim(0.8, 2.5)\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[1].set_color('#e74c3c')\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "# draw vertical lines to separate the rowhammer threshold values\n",
    "ax.axvline(x=0.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=1.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=2.5, color='grey', linestyle='-', alpha=0.5)\n",
    "# make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(16)\n",
    "ax.yaxis.label.set_size(16)\n",
    "\n",
    "# put the legend on top of the plot\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=5, fancybox=True, shadow=True, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_energy_comparison_single_core.pdf', bbox_inches='tight')\n",
    "# export data to csv\n",
    "stats_no_baseline.to_csv('abacus_energy_comparison_single_core.csv', index=False)\n",
    "\n",
    "# REGA at 1000 nRH normalized_energy for the geomean workload\n",
    "print(stats_no_baseline[(stats_no_baseline['nrh'] == 1000) & (stats_no_baseline['config'] == 'REGA')]['normalized_energy'].mean()/stats_no_baseline[(stats_no_baseline['nrh'] == 1000) & (stats_no_baseline['config'] == 'ABACuS')]['normalized_energy'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy comparison (8-core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MECHANISM_NAME = 'ABACuS'\n",
    "\n",
    "### READ RESULTS INTO PANDAS DATAFRAME\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsdir = \"../results\"\n",
    "# list all directories in resultsdir\n",
    "configs = [d for d in os.listdir(resultsdir) if os.path.isdir(os.path.join(resultsdir, d))]\n",
    "configs = ['Baseline.yaml', 'MC-Baseline.yaml', 'Hydra-Baseline.yaml', 'REGA125.yaml', 'REGA250.yaml', 'REGA500.yaml', 'REGA1000.yaml', 'Graphene125.yaml', 'Graphene250.yaml', 'Graphene500.yaml', 'Graphene1000.yaml', 'Hydra125.yaml', 'Hydra250.yaml', 'Hydra500.yaml', 'Hydra1000.yaml', 'PARA125.yaml', 'PARA250.yaml', 'PARA500.yaml', 'PARA1000.yaml', 'ABACUS125.yaml', 'ABACUS250.yaml', 'ABACUS500.yaml', 'ABACUS1000.yaml']\n",
    "\n",
    "# print found configs\n",
    "print('Found configs: {}'.format(configs))\n",
    "# list all directories under all configs\n",
    "workloads = []\n",
    "for c in configs:\n",
    "    workloads.append([d for d in os.listdir(os.path.join(resultsdir, c)) if os.path.isdir(os.path.join(resultsdir, c, d))])\n",
    "# find only the intersection of all workloads\n",
    "# workloads = list(set.intersection(*map(set, workloads)))\n",
    "workloads = list(set([item for sublist in workloads for item in sublist]))\n",
    "# print found workloads\n",
    "print('Found workloads: {}'.format(workloads))\n",
    "\n",
    "stats_per_config_workload = []\n",
    "\n",
    "# remove workloads with fewer than 7 dashes\n",
    "workloads = [w for w in workloads if w.count('-') >= 7]\n",
    "\n",
    "# for every config + workload directory\n",
    "for c in configs:\n",
    "    for w in workloads:\n",
    "        # find all files in the directory\n",
    "        if os.path.isdir(os.path.join(resultsdir, c, w)):\n",
    "            files = [f for f in os.listdir(os.path.join(resultsdir, c, w)) if os.path.isfile(os.path.join(resultsdir, c, w, f))]\n",
    "            # find the stats file\n",
    "            stat_files = [f for f in files if f.endswith('.stats')]\n",
    "            # if there is a stats file\n",
    "            if stat_files:\n",
    "                for stat_file in stat_files:\n",
    "                    # if the stats_file has less than three lines skip it\n",
    "                    if len(open(os.path.join(resultsdir, c, w, stat_file)).readlines()) < 3:\n",
    "                        continue\n",
    "                    \n",
    "                    # print the name of the stats_file\n",
    "                    print('Found stats file: {}'.format(os.path.join(os.path.join(resultsdir, c, w, stat_file))))\n",
    "\n",
    "                    extension = ''\n",
    "                    # if stats_file file name itself does not start with DDR4, parse it a bit\n",
    "                    if not stat_file.startswith('DDR4'):\n",
    "                        # get the config name from the stats_file name\n",
    "                        extension = '_'.join(stat_file.split('_')[:-1])\n",
    "                        # prepend underscore to extension\n",
    "                        extension = '_' + extension\n",
    "\n",
    "                    # read the stats file, name columns: 'name', 'value', 'description'\n",
    "                    df = pd.read_csv(os.path.join(resultsdir, c, w, stat_file), header=None).T\n",
    "                    df.columns = df.iloc[0]\n",
    "\n",
    "                    # if df has this column total_dram_energy0_channel\n",
    "                    if 'ramulator.total_dram_energy0_channel' in df.columns:\n",
    "                        total_energy = df['ramulator.total_dram_energy0_channel'].values[1]\n",
    "                    else:\n",
    "                        total_energy = 0\n",
    "\n",
    "                    # create a df with the config, workload and total_energy\n",
    "                    df = pd.DataFrame({'config': [c], 'workload': [w], 'total_energy': [total_energy]})\n",
    "                    df.reset_index(inplace=True, drop=True)\n",
    "                    stats_per_config_workload.append(df)\n",
    "            else:\n",
    "                print('Config: {}, Workload: {}, Stats: No stats file found'.format(c, w))\n",
    "\n",
    "\n",
    "# concatenate all stats into one dataframe\n",
    "stats = pd.concat(stats_per_config_workload)\n",
    "\n",
    "\n",
    "# remove workloads that have \"gups\" in them\n",
    "stats = stats[~stats['workload'].str.contains('gups')]\n",
    "\n",
    "# grep_map0 produced 0 cycles for any core except 7, weird bug, ignore for now\n",
    "# stats = stats[~stats['workload'].str.contains('grep_map0')]\n",
    "\n",
    "# also remove them from the list\n",
    "workloads = [w for w in workloads if 'gups' not in w]\n",
    "\n",
    "# grep_map0 produced 0 cycles for any core except 7, weird bug, ignore for now\n",
    "# workloads = [w for w in workloads if 'grep_map0' not in w]\n",
    "\n",
    "\n",
    "# replace all instances of \"random_10.trace\" in workload names with \"gups\"\n",
    "stats['workload'] = stats['workload'].str.replace('random_10.trace', 'gups')\n",
    "HIGH_RBMPKI = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "stats = stats[stats['workload'].str.contains('|'.join(HIGH_RBMPKI))]\n",
    "# remove h264_decode fro workloads\n",
    "# stats = stats[~stats['workload'].str.contains('h264_decode')]\n",
    "# remove h264_decode from HIGH_RBMPKI\n",
    "# HIGH_RBMPKI.remove('h264_decode')\n",
    "\n",
    "# remove from mc_only_stats the workloads that contain less than 7 dashes\n",
    "stats = stats[stats['workload'].str.count('-') >= 7]\n",
    "\n",
    "# print number of unique workloads\n",
    "print('Unique workloads: {}'.format(stats['workload'].nunique()))\n",
    "\n",
    "# keep mc_only_stats only for workloads that contain the strings in HIGH_RBMPKI\n",
    "\n",
    "# remove \"-16DR\" from config names\n",
    "stats['config'] = stats['config'].str.replace('-16DR', '')\n",
    "\n",
    "# replace 1K with 1000 in config names\n",
    "stats['config'] = stats['config'].str.replace('1K', '1000')\n",
    "\n",
    "# replace 'Baseline' with 'Baseline0'\n",
    "stats['config'] = stats['config'].str.replace('Baseline', 'Baseline0')\n",
    "\n",
    "# add a new column that stores in integer the number in the config name\n",
    "stats['nrh'] = stats['config'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# remove numbers from config names\n",
    "stats['config'] = stats['config'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# remove yaml from config names\n",
    "stats['config'] = stats['config'].str.replace('.yaml', '')\n",
    "\n",
    "# replace SPR with APAR\n",
    "stats['config'] = stats['config'].str.replace('ABACUS', MY_MECHANISM_NAME)\n",
    "\n",
    "stats_copy = stats.copy()\n",
    "\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=5)\n",
    "\n",
    "stats = stats_copy.copy()\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline = stats[stats['config'] == 'Baseline']\n",
    "baseline = baseline[['workload', 'total_energy']]\n",
    "# baseline\n",
    "baseline.columns = ['workload', 'baseline_energy']\n",
    "stats = pd.merge(stats, baseline, on='workload')\n",
    "\n",
    "#hydra baseline\n",
    "hydra_baseline = stats[stats['config'] == 'Hydra-Baseline']\n",
    "hydra_baseline = hydra_baseline[['workload', 'total_energy']]\n",
    "# hydra_baseline\n",
    "hydra_baseline.columns = ['workload', 'ramulator.hydra_baseline_energy']\n",
    "stats = pd.merge(stats, hydra_baseline, on='workload')\n",
    "\n",
    "stats['normalized_energy'] = stats['total_energy'] / stats['baseline_energy']\n",
    "\n",
    "# remove normalized_energy below 1\n",
    "stats = stats[stats['normalized_energy'] >= 0.9]\n",
    "# remove 429.mcf\n",
    "# stats = stats[~stats['workload'].str.contains('bfs_ny')]\n",
    "# stats = stats[~stats['workload'].str.contains('bfs_cm2003')]\n",
    "\n",
    "# normalized ipc for hydra is not correct, so we overwrite it with the correct value\n",
    "stats.loc[stats['config'].str.contains('Hydra'), 'normalized_energy'] = stats['total_energy'] / stats['ramulator.hydra_baseline_energy']\n",
    "\n",
    "# new dataframe that does not have the baseline configs\n",
    "stats_no_baseline = stats[~stats['config'].str.contains('Baseline')]\n",
    "\n",
    "# new dataframe that does not have the baseline configs\n",
    "print(stats_no_baseline['config'].unique())\n",
    "\n",
    "# order nRH from high to low\n",
    "stats_no_baseline['nrh'] = pd.Categorical(stats_no_baseline['nrh'], categories=[1000, 500, 250, 125], ordered=True)\n",
    "\n",
    "# order config in this order: abacus, Graphene, Hydra, REGA, PARA\n",
    "stats_no_baseline['config'] = pd.Categorical(stats_no_baseline['config'], categories=['ABACuS', 'Graphene', 'Hydra', 'REGA', 'PARA'], ordered=True)\n",
    "\n",
    "#boxplot of normalized IPC\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "# show mean values as well\n",
    "ax = sns.boxplot(x=\"nrh\", y=\"normalized_energy\", hue=\"config\", data=stats_no_baseline, showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\"})#, showfliers = False)\n",
    "ax.set_xlabel('RowHammer Threshold ($N_{RH}$)')\n",
    "ax.set_ylabel('Normalized Energy Distribution')\n",
    "# draw a red line at y = 1.0, label it as baseline IPC\n",
    "ax.axhline(y=1.0, color='r', linestyle='--')\n",
    "# write above the red line 'baseline IPC'\n",
    "ax.text(0.02, 0.02, 'Baseline DRAM energy', color='#e74c3c', transform=ax.transAxes, fontsize=15)\n",
    "# extend the y axis to 1.2\n",
    "ax.set_ylim(0.6, 4.0)\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[1].set_color('#e74c3c')\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "# draw vertical lines to separate the rowhammer threshold values\n",
    "ax.axvline(x=0.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=1.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=2.5, color='grey', linestyle='-', alpha=0.5)\n",
    "# make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(16)\n",
    "ax.yaxis.label.set_size(16)\n",
    "\n",
    "# put the legend on top of the plot\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=5, fancybox=True, shadow=True, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_energy_comparison_multi_core.pdf', bbox_inches='tight')\n",
    "# export data to csv\n",
    "stats_no_baseline.to_csv('abacus_energy_comparison_multi_core.csv', index=False)\n",
    "\n",
    "# print mean DRAM energy for ABAcus at each nRH\n",
    "print(stats_no_baseline[stats_no_baseline['config'] == 'ABACuS'].groupby('nrh')['normalized_energy'].mean())\n",
    "print(stats_no_baseline[stats_no_baseline['config'] == 'ABACuS'].groupby('nrh')['normalized_energy'].max())\n",
    "\n",
    "print(1-stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 125)]['normalized_energy'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'Hydra') & (stats_no_baseline['nrh'] == 125)]['normalized_energy'].mean())\n",
    "print(1-stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 125)]['normalized_energy'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'REGA') & (stats_no_baseline['nrh'] == 125)]['normalized_energy'].mean())\n",
    "print(1-stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 125)]['normalized_energy'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'PARA') & (stats_no_baseline['nrh'] == 125)]['normalized_energy'].mean())\n",
    "\n",
    "print(1-stats_no_baseline[(stats_no_baseline['config'] == 'Graphene') & (stats_no_baseline['nrh'] == 125)]['normalized_energy'].mean()/stats_no_baseline[(stats_no_baseline['config'] == 'ABACuS') & (stats_no_baseline['nrh'] == 125)]['normalized_energy'].mean())\n",
    "\n",
    "\n",
    "# print the number of unique workloads\n",
    "print('Unique workloads: {}'.format(stats_no_baseline['workload'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MECHANISM_NAME = 'ABACuS'\n",
    "\n",
    "### READ RESULTS INTO PANDAS DATAFRAME\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "resultsdir = \"../results\"\n",
    "# list all directories in resultsdir\n",
    "configs = [d for d in os.listdir(resultsdir) if os.path.isdir(os.path.join(resultsdir, d))]\n",
    "configs = ['Baseline-AH.yaml', 'Baseline-AAH.yaml', 'Baseline-RH32.yaml', 'ABACUS500-RH32.yaml', 'ABACUS500-AH.yaml', 'ABACUS500-AAH.yaml', 'Graphene500-AH.yaml', 'Graphene500-AAH.yaml','Graphene500-RH32.yaml', 'REGA500-RH32.yaml', 'REGA500-AH.yaml', 'REGA500-AAH.yaml','PARA500-AH.yaml', 'PARA500-AAH.yaml', 'PARA500-RH32.yaml', 'Hydra500-AH.yaml', 'Hydra500-AAH.yaml','Hydra500-RH32.yaml', 'ABACUS500-RH32-Big.yaml', 'ABACUS500-AH-Big.yaml', 'ABACUS500-AAH-Big.yaml']\n",
    "# print found configs\n",
    "print('Found configs: {}'.format(configs))\n",
    "# list all directories under all configs\n",
    "workloads = []\n",
    "for c in configs:\n",
    "    workloads.append([d for d in os.listdir(os.path.join(resultsdir, c)) if os.path.isdir(os.path.join(resultsdir, c, d))])\n",
    "# find only the intersection of all workloads\n",
    "workloads = list(set.intersection(*map(set, workloads)))\n",
    "# print found workloads\n",
    "print('Found workloads: {}'.format(workloads))\n",
    "\n",
    "stats_per_config_workload = []\n",
    "\n",
    "# for every config + workload directory\n",
    "for c in configs:\n",
    "    for w in workloads:\n",
    "        # find all files in the directory\n",
    "        files = [f for f in os.listdir(os.path.join(resultsdir, c, w)) if os.path.isfile(os.path.join(resultsdir, c, w, f))]\n",
    "        # find the stats file\n",
    "        stat_files = [f for f in files if f.endswith('.stats')]\n",
    "        # if there is a stats file\n",
    "        if stat_files:\n",
    "            for stat_file in stat_files:\n",
    "                # if the stats_file has less than three lines skip it\n",
    "                if len(open(os.path.join(resultsdir, c, w, stat_file)).readlines()) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # print the name of the stats_file\n",
    "                print('Found stats file: {}'.format(os.path.join(os.path.join(resultsdir, c, w, stat_file))))\n",
    "\n",
    "                extension = ''\n",
    "                # if stats_file file name itself does not start with DDR4, parse it a bit\n",
    "                if not stat_file.startswith('DDR4'):\n",
    "                    # get the config name from the stats_file name\n",
    "                    extension = '_'.join(stat_file.split('_')[:-1])\n",
    "                    # prepend underscore to extension\n",
    "                    extension = '_' + extension\n",
    "\n",
    "                # read the stats file, name columns: 'name', 'value', 'description'\n",
    "                df = pd.read_csv(os.path.join(resultsdir, c, w, stat_file), header=None).T\n",
    "                df.columns = df.iloc[0]\n",
    "                df.drop(0,inplace=True)\n",
    "                # add a new column called 'config' with the config name\n",
    "                df['config'] = c + extension\n",
    "                # add a new column called 'workload' with the workload name\n",
    "                df['workload'] = w\n",
    "                # print the stats file\n",
    "                # print('Config: {}, Workload: {}, Stats: {}'.format(c, w, df))\n",
    "                # append the stats to the list\n",
    "                df.reset_index(inplace=True, drop=True)\n",
    "                stats_per_config_workload.append(df)\n",
    "        else:\n",
    "            print('Config: {}, Workload: {}, Stats: No stats file found'.format(c, w))\n",
    "\n",
    "# concatenate all stats into one dataframe\n",
    "stats = pd.concat(stats_per_config_workload)\n",
    "\n",
    "# find elements where workload does not contain '-'\n",
    "# these are multi core workloads\n",
    "stats = stats[~stats['workload'].str.contains('-')]\n",
    "\n",
    "# remove these two workloads: stream_10.trace and random_10.trace\n",
    "stats = stats[~stats['workload'].isin(['gups'])]\n",
    "# also from workloads\n",
    "workloads = [w for w in workloads if not w in ['gups']]\n",
    "\n",
    "# remove \"-16DR\" from config names\n",
    "stats['config'] = stats['config'].str.replace('-16DR', '')\n",
    "\n",
    "# replace 1K with 1000 in config names\n",
    "stats['config'] = stats['config'].str.replace('1K', '1000')\n",
    "\n",
    "# replace 'Baseline' with 'Baseline0'\n",
    "stats['config'] = stats['config'].str.replace('Baseline', 'Baseline0')\n",
    "\n",
    "# add a new column that stores in integer the number in the config name\n",
    "stats['nrh'] = stats['config'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# remove numbers from config names\n",
    "stats['config'] = stats['config'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# remove yaml from config names\n",
    "stats['config'] = stats['config'].str.replace('.yaml', '')\n",
    "\n",
    "# replace SPR with APAR\n",
    "stats['config'] = stats['config'].str.replace('ABACUS', MY_MECHANISM_NAME)\n",
    "\n",
    "stats.loc[stats['workload'] == 'random_10.trace', 'workload'] = 'gups'\n",
    "\n",
    "# increasing order of rbmpki\n",
    "# order = ['511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', '500.perlbench', '523.xalancbmk', '510.parest', '557.xz', '482.sphinx3', '505.mcf', '436.cactusADM', '471.omnetpp', '473.astar', '483.xalancbmk', '462.libquantum', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf']\n",
    "order = ['h264_encode', '511.povray', '481.wrf', '541.leela', '538.imagick', '444.namd', '447.dealII', '464.h264ref', '456.hmmer', '403.gcc', '526.blender', '544.nab', '525.x264', '508.namd', 'grep_map0', '531.deepsjeng', '458.sjeng', '435.gromacs', '445.gobmk', '401.bzip2', '507.cactuBSSN', '502.gcc', 'ycsb_abgsave', 'tpch6', '500.perlbench', '523.xalancbmk', 'ycsb_dserver', 'ycsb_cserver', '510.parest', 'ycsb_bserver', 'ycsb_eserver', 'stream_10.trace', 'tpcc64', 'ycsb_aserver', '557.xz', '482.sphinx3', 'jp2_decode', '505.mcf', 'wc_8443', 'wc_map0', '436.cactusADM', '471.omnetpp', '473.astar', 'jp2_encode', 'tpch17', '483.xalancbmk', '462.libquantum', 'tpch2', '433.milc', '520.omnetpp', '437.leslie3d', '450.soplex', '459.GemsFDTD', '549.fotonik3d', '434.zeusmp', '519.lbm', '470.lbm', '429.mcf', 'gups', 'h264_decode', 'bfs_ny', 'bfs_cm2003', 'bfs_dblp']\n",
    "\n",
    "# remove all workloads not in order\n",
    "stats = stats[stats['workload'].isin(order)]\n",
    "# also from the workload list\n",
    "workloads = [w for w in workloads if w in order]\n",
    "\n",
    "# order workloads according to the order\n",
    "stats['workload'] = pd.Categorical(stats['workload'], categories=order, ordered=True)\n",
    "\n",
    "stats_copy = stats.copy()\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=4)\n",
    "\n",
    "stats = stats_copy.copy()\n",
    "\n",
    "# instructions per cycle (IPC) is record_cycles_insts_0 / record_cycs_core_0\n",
    "stats['ramulator.ipc'] = stats['ramulator.record_insts_core_0'] / stats['ramulator.record_cycs_core_0']\n",
    "\n",
    "\n",
    "stats['ramulator.rbmpki'] = (stats['ramulator.row_conflicts_channel_0_core'] + stats['ramulator.row_misses_channel_0_core']) /\\\n",
    "                            stats['ramulator.record_insts_core_0'] * 1000\n",
    "\n",
    "# if ramulator.row_count_cache_evictions_channel_0 is zero, then make it 1 billion\n",
    "stats.loc[stats['ramulator.row_count_cache_evictions_channel_0'] == 0, 'ramulator.row_count_cache_evictions_channel_0'] = 1000000000\n",
    "\n",
    "stats['ramulator.rccepllcm'] = stats['ramulator.L3_cache_total_miss'] / stats['ramulator.row_count_cache_evictions_channel_0']\n",
    "\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline_ah = stats[stats['config'] == 'Baseline-AH']\n",
    "baseline_ah = baseline_ah[['workload', 'ramulator.ipc']]\n",
    "# baseline\n",
    "baseline_ah.columns = ['workload', 'ramulator.baseline_ipc_ah']\n",
    "\n",
    "baseline_rh = stats[stats['config'] == 'Baseline-RH']\n",
    "baseline_rh = baseline_rh[['workload', 'ramulator.ipc']]\n",
    "# baseline\n",
    "baseline_rh.columns = ['workload', 'ramulator.baseline_ipc_rh']\n",
    "\n",
    "baseline_aah = stats[stats['config'] == 'Baseline-AAH']\n",
    "baseline_aah = baseline_aah[['workload', 'ramulator.ipc']]\n",
    "# baseline\n",
    "baseline_aah.columns = ['workload', 'ramulator.baseline_ipc_aah']\n",
    "\n",
    "stats = pd.merge(stats, baseline_ah, on='workload')\n",
    "stats = pd.merge(stats, baseline_rh, on='workload')\n",
    "stats = pd.merge(stats, baseline_aah, on='workload')\n",
    "\n",
    "stats['ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc_rh']\n",
    "\n",
    "stats.loc[stats['config'].str.contains('-AH'), 'ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc_ah']\n",
    "stats.loc[stats['config'].str.contains('-AAH'), 'ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc_aah']\n",
    "\n",
    "\n",
    "\n",
    "# add a new column called attack workload \n",
    "# if config contains '-RH' then the attack workload is RH\n",
    "# if config contains '-AH' then the attack workload is AP\n",
    "# if config contains '-AAH' then the attack workload is HAP\n",
    "\n",
    "stats['attack_workload'] = 'RowHammer Attack'\n",
    "stats.loc[stats['config'].str.contains('-AH'), 'attack_workload'] = 'Hydra-Adversarial'\n",
    "stats.loc[stats['config'].str.contains('-AAH'), 'attack_workload'] = 'ABACuS-Adversarial'\n",
    "\n",
    "# order attack_workload\n",
    "stats['attack_workload'] = pd.Categorical(stats['attack_workload'], categories=['RowHammer Attack', 'Hydra-Adversarial', 'ABACuS-Adversarial'], ordered=True)\n",
    "\n",
    "# remove -RH -AH -AAH from config\n",
    "stats['config'] = stats['config'].str.replace('-RH', '')\n",
    "stats['config'] = stats['config'].str.replace('-AH', '')\n",
    "stats['config'] = stats['config'].str.replace('-AAH', '')\n",
    "\n",
    "\n",
    "# remove baseline\n",
    "stats = stats[~stats['config'].str.contains('Baseline')]\n",
    "\n",
    "# order configs\n",
    "stats['config'] = pd.Categorical(stats['config'], categories=['ABACuS', 'Graphene', 'Hydra', 'REGA', 'PARA'], ordered=True)\n",
    "\n",
    "# if config is Hydra and attack_workload is Hydra Adversarial Pattern AP then\n",
    "\n",
    "#boxplot of normalized IPC\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "# show mean values as well\n",
    "ax = sns.boxplot(x=\"attack_workload\", y=\"ramulator.normalized_ipc\", hue=\"config\", data=stats, showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\"})\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Normalized IPC Distribution')\n",
    "# draw a red line at y = 1.0, label it as baseline IPC\n",
    "ax.axhline(y=1.0, color='r', linestyle='--')\n",
    "# write above the red line 'baseline IPC'\n",
    "ax.text(0.02, 0.93, 'Baseline IPC', color='#e74c3c', transform=ax.transAxes, fontsize=15)\n",
    "# extend the y axis to 1.2\n",
    "ax.set_ylim(0, 1.1)\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[5].set_color('#e74c3c')\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "# draw vertical lines to separate the rowhammer threshold values\n",
    "ax.axvline(x=0.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=1.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=2.5, color='grey', linestyle='-', alpha=0.5)\n",
    "# make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(16)\n",
    "ax.yaxis.label.set_size(16)\n",
    "\n",
    "# put the legend on top of the plot\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=20, fancybox=True, shadow=True, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_adversarial.pdf', bbox_inches='tight')\n",
    "\n",
    "# use seaborn-deep style\n",
    "sns.set(font_scale=1.0)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"pastel\", n_colors=4)\n",
    "\n",
    "stats = stats_copy.copy()\n",
    "\n",
    "# instructions per cycle (IPC) is record_cycles_insts_0 / record_cycs_core_0\n",
    "stats['ramulator.ipc'] = stats['ramulator.record_insts_core_0'] / stats['ramulator.record_cycs_core_0']\n",
    "\n",
    "\n",
    "stats['ramulator.rbmpki'] = (stats['ramulator.row_conflicts_channel_0_core'] + stats['ramulator.row_misses_channel_0_core']) /\\\n",
    "                            stats['ramulator.record_insts_core_0'] * 1000\n",
    "\n",
    "# if ramulator.row_count_cache_evictions_channel_0 is zero, then make it 1 billion\n",
    "stats.loc[stats['ramulator.row_count_cache_evictions_channel_0'] == 0, 'ramulator.row_count_cache_evictions_channel_0'] = 1000000000\n",
    "\n",
    "stats['ramulator.rccepllcm'] = stats['ramulator.L3_cache_total_miss'] / stats['ramulator.row_count_cache_evictions_channel_0']\n",
    "\n",
    "\n",
    "# copy the IPC of the baseline config as to all configs\n",
    "baseline_ah = stats[stats['config'] == 'Baseline-AH']\n",
    "baseline_ah = baseline_ah[['workload', 'ramulator.ipc']]\n",
    "# baseline\n",
    "baseline_ah.columns = ['workload', 'ramulator.baseline_ipc_ah']\n",
    "\n",
    "baseline_rh = stats[stats['config'] == 'Baseline-RH']\n",
    "baseline_rh = baseline_rh[['workload', 'ramulator.ipc']]\n",
    "# baseline\n",
    "baseline_rh.columns = ['workload', 'ramulator.baseline_ipc_rh']\n",
    "\n",
    "baseline_aah = stats[stats['config'] == 'Baseline-AAH']\n",
    "baseline_aah = baseline_aah[['workload', 'ramulator.ipc']]\n",
    "# baseline\n",
    "baseline_aah.columns = ['workload', 'ramulator.baseline_ipc_aah']\n",
    "\n",
    "stats = pd.merge(stats, baseline_ah, on='workload')\n",
    "stats = pd.merge(stats, baseline_rh, on='workload')\n",
    "stats = pd.merge(stats, baseline_aah, on='workload')\n",
    "\n",
    "stats['ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc_rh']\n",
    "\n",
    "stats.loc[stats['config'].str.contains('-AH'), 'ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc_ah']\n",
    "stats.loc[stats['config'].str.contains('-AAH'), 'ramulator.normalized_ipc'] = stats['ramulator.ipc'] / stats['ramulator.baseline_ipc_aah']\n",
    "\n",
    "\n",
    "\n",
    "# add a new column called attack workload \n",
    "# if config contains '-RH' then the attack workload is RH\n",
    "# if config contains '-AH' then the attack workload is AP\n",
    "# if config contains '-AAH' then the attack workload is HAP\n",
    "\n",
    "stats['attack_workload'] = 'RowHammer Attack'\n",
    "stats.loc[stats['config'].str.contains('-AH'), 'attack_workload'] = 'Hydra-Adversarial'\n",
    "stats.loc[stats['config'].str.contains('-AAH'), 'attack_workload'] = 'ABACuS-Adversarial'\n",
    "\n",
    "# order attack_workload\n",
    "stats['attack_workload'] = pd.Categorical(stats['attack_workload'], categories=['RowHammer Attack', 'Hydra-Adversarial', 'ABACuS-Adversarial'], ordered=True)\n",
    "\n",
    "# remove -RH -AH -AAH from config\n",
    "stats['config'] = stats['config'].str.replace('-RH', '')\n",
    "stats['config'] = stats['config'].str.replace('-AH', '')\n",
    "stats['config'] = stats['config'].str.replace('-AAH', '')\n",
    "\n",
    "\n",
    "# remove baseline\n",
    "stats = stats[~stats['config'].str.contains('Baseline')]\n",
    "\n",
    "# order configs\n",
    "stats['config'] = pd.Categorical(stats['config'], categories=['ABACuS', 'ABACuS-Big'], ordered=True)\n",
    "\n",
    "# if config is Hydra and attack_workload is Hydra Adversarial Pattern AP then\n",
    "\n",
    "#boxplot of normalized IPC\n",
    "fig, ax = plt.subplots(figsize=(7, 2))\n",
    "# show mean values as well\n",
    "ax = sns.boxplot(x=\"attack_workload\", y=\"ramulator.normalized_ipc\", hue=\"config\", data=stats, showmeans=True, meanprops={\"marker\":\"o\",\"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\"})\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Normalized IPC\\nDistribution')\n",
    "# draw a red line at y = 1.0, label it as baseline IPC\n",
    "ax.axhline(y=1.0, color='r', linestyle='--')\n",
    "# write above the red line 'baseline IPC'\n",
    "ax.text(0.02, 0.82, 'Baseline IPC', color='#e74c3c', transform=ax.transAxes, fontsize=15)\n",
    "# extend the y axis to 1.2\n",
    "ax.set_ylim(0.7, 1.1)\n",
    "# color the 5th y tick red\n",
    "ax.get_yticklabels()[3].set_color('#e74c3c')\n",
    "# make axis tick font bigger\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "# draw vertical lines to separate the rowhammer threshold values\n",
    "ax.axvline(x=0.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=1.5, color='grey', linestyle='-', alpha=0.5)\n",
    "ax.axvline(x=2.5, color='grey', linestyle='-', alpha=0.5)\n",
    "# make x and y axis labels bigger\n",
    "ax.xaxis.label.set_size(12)\n",
    "ax.yaxis.label.set_size(16)\n",
    "\n",
    "# maxe x axis labels smaller\n",
    "ax.xaxis.set_tick_params(labelsize=12)\n",
    "\n",
    "# put the legend on top of the plot\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.65, 1.15), ncol=20, fancybox=True, shadow=True, fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig.savefig('abacus_adversarial_big.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# print mean normalized ipc of ABACUS in rowhammer attack\n",
    "print('ABACuS mean normalized IPC in RowHammer attack: ', 1-(stats.loc[(stats['config'] == 'ABACuS') & (stats['attack_workload'] == 'RowHammer Attack'), 'ramulator.normalized_ipc'].mean()))\n",
    "print('ABACuS-Big mean normalized IPC in RowHammer attack: ', 1-(stats.loc[(stats['config'] == 'ABACuS-Big') & (stats['attack_workload'] == 'RowHammer Attack'), 'ramulator.normalized_ipc'].mean()))\n",
    "\n",
    "\n",
    "print('ABACuS mean normalized IPC in HydraADV: ', 1-(stats.loc[(stats['config'] == 'ABACuS') & (stats['attack_workload'] == 'Hydra-Adversarial'), 'ramulator.normalized_ipc'].mean()))\n",
    "print('ABACuS-Big mean normalized IPC in HydraADV: ', 1-(stats.loc[(stats['config'] == 'ABACuS-Big') & (stats['attack_workload'] == 'Hydra-Adversarial'), 'ramulator.normalized_ipc'].mean()))\n",
    "\n",
    "\n",
    "print('ABACuS mean normalized IPC in ABACuSADV: ', 1-(stats.loc[(stats['config'] == 'ABACuS') & (stats['attack_workload'] == 'ABACuS-Adversarial'), 'ramulator.normalized_ipc'].mean()))\n",
    "print('ABACuS-Big mean normalized IPC in ABACuSADV: ', 1-(stats.loc[(stats['config'] == 'ABACuS-Big') & (stats['attack_workload'] == 'ABACuS-Adversarial'), 'ramulator.normalized_ipc'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRR Analysis Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counters = [16, 32, 64, 96, 128, 164]\n",
    "N_RH = [70070, 35490, 17290, 10920, 8190, 6370]\n",
    "\n",
    "# whitegrid style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# colorpalette with 6 colors\n",
    "# first color is blue\n",
    "# other colors are red\n",
    "colors = sns.color_palette(sns.color_palette('Blues', 1) + 5 * sns.color_palette('Greens', 1))\n",
    "\n",
    "myblue = sns.color_palette('Blues', 1)[0]\n",
    "mygreen = sns.color_palette('Greens', 1)[0]\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# plot n_rh by counters as barplot\n",
    "sns.barplot(x=counters, y=N_RH, palette=colors)\n",
    "\n",
    "# make bars thinner and add black border\n",
    "for patch in plt.gca().patches:\n",
    "    patch.set_linewidth(1.0)\n",
    "    patch.set_edgecolor('black')\n",
    "\n",
    "# draw a horizontal line at y = 4800\n",
    "plt.axhline(y=4800, color='red', linestyle='--')\n",
    "\n",
    "# add a y axis label at 4800 (modify axis labels) NOT BY ADDING TEXT TO THE PLOT\n",
    "plt.yticks([4800] + list(range(10000, 80000, 10000)), ['4.8K'] + list(str(x) + 'K' for x in range(10, 80, 10)))\n",
    "\n",
    "# y axis label is $N_{RH}$\n",
    "plt.ylabel('RowHammer Threshold ($N_{RH}$)', fontsize=12)\n",
    "\n",
    "# x axis label is Number of counters\n",
    "plt.xlabel('Number of counters', fontsize=12)\n",
    "\n",
    "# increase font isze of x and y axis labels\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "# annotate the horizontal red line \"Minimum observed RowHammer Threshold\"\n",
    "plt.annotate('', xy=(0.7, 4800), xytext=(0.7, 15000),\n",
    "             arrowprops=dict(facecolor='red', shrink=0.05, width=1, headwidth=5,edgecolor='red'),\n",
    "             horizontalalignment='center')\n",
    "\n",
    "# add text inside a semi-transparent box\n",
    "plt.text(0.21, 0.3, 'Minimum observed\\nRowHammer Threshold', size=8, weight='bold', color='red', horizontalalignment='center', verticalalignment='center',\n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.9, edgecolor='red'))\n",
    "\n",
    "plt.text(0.211, 0.8, 'Reverse engineered (in [55]) TRR configuration', weight='bold', size=8, color=myblue, horizontalalignment='left', verticalalignment='center',\n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.9, edgecolor=myblue))\n",
    "\n",
    "plt.text(0.30, 0.66, 'Future configurations with more counters', weight='bold', size=8, color=mygreen, horizontalalignment='left', verticalalignment='center',\n",
    "         transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.9, edgecolor=mygreen))\n",
    "\n",
    "# save figure to pdf 'trr-thresholds.pdf'\n",
    "plt.savefig('trr-thresholds.pdf', bbox_inches='tight')\n",
    "\n",
    "# above annotation but text centered\n",
    "# plt.annotate('Minimum observed\\nRowHammer Threshold', xy=(0, 4800), xytext=(1, 45000),\n",
    "#              arrowprops=dict(facecolor='red', shrink=0.05, width=1, headwidth=5,edgecolor='red'),\n",
    "#              horizontalalignment='center')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
